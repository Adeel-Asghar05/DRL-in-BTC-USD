{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f00cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# =========================\n",
    "# Custom callback for printing during training\n",
    "# =========================\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.episode = 0\n",
    "        self.num_correct = 0\n",
    "        self.total_steps = 0\n",
    "        self.current_ep_reward = 0\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        info = self.locals['infos'][0]\n",
    "        action = self.locals['actions'][0]\n",
    "        reward = self.locals['rewards'][0]\n",
    "        done = self.locals['dones'][0]\n",
    "\n",
    "        step = info['step']\n",
    "        delta = info['delta']\n",
    "        delta_pct = info['delta_pct'] * 100  # convert to % for readability\n",
    "\n",
    "        # Map action to string\n",
    "        action_str = ['Hold', 'Buy', 'Sell'][action]\n",
    "\n",
    "        # Movement string\n",
    "        if abs(delta_pct) < info['threshold_pct'] * 100:\n",
    "            movement = 'stable'\n",
    "        elif delta_pct > 0:\n",
    "            movement = 'up'\n",
    "        else:\n",
    "            movement = 'down'\n",
    "        movement_str = f\"{movement} ({delta_pct:.2f}%)\"\n",
    "\n",
    "        print(f\"Step {step}, chosen action {action_str}, next price movement {movement_str}, reward {reward:.4f}\")\n",
    "\n",
    "        # Track accuracy (reward >= 0 considered correct)\n",
    "        self.total_steps += 1\n",
    "        if reward >= 0:\n",
    "            self.num_correct += 1\n",
    "        accuracy = (self.num_correct / self.total_steps) * 100\n",
    "        print(f\"Running accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        self.current_ep_reward += reward\n",
    "\n",
    "        if step == 0:\n",
    "            print(f\"Episode {self.episode + 1}\")\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode {self.episode + 1} completed.\")\n",
    "            self.episode_rewards.append(self.current_ep_reward)\n",
    "            self.current_ep_reward = 0\n",
    "            self.episode += 1\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Custom BTC Trading Environment\n",
    "# =========================\n",
    "class BTCTradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(BTCTradingEnv, self).__init__()\n",
    "        self.data = data.drop(columns=['OpenTime'])\n",
    "        self.data = self.data.astype(np.float32)\n",
    "\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(9,), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(3)  # Hold, Buy, Sell\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.current_step = 0\n",
    "        obs = self._get_obs()\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        current_close = self.data.iloc[self.current_step]['Close']\n",
    "        next_close = self.data.iloc[self.current_step + 1]['Close']\n",
    "        delta = next_close - current_close\n",
    "        delta_pct = delta / current_close  # percentage change\n",
    "\n",
    "        # Threshold for \"stable\" (0.1%)\n",
    "        threshold_pct = 0.001\n",
    "\n",
    "        # =========================\n",
    "        # Reward system (improved)\n",
    "        # =========================\n",
    "        if action == 0:  # Hold\n",
    "            if abs(delta_pct) < threshold_pct:\n",
    "                reward = 0.05  # small positive reward for staying safe\n",
    "            else:\n",
    "                reward = -abs(delta_pct)  # penalty for missing a move\n",
    "\n",
    "        elif action == 1:  # Buy\n",
    "            reward = delta_pct  # gain/loss proportional to % change\n",
    "\n",
    "        elif action == 2:  # Sell\n",
    "            reward = -delta_pct  # profit if price goes down\n",
    "\n",
    "        # Info dict\n",
    "        info = {\n",
    "            'step': self.current_step,\n",
    "            'delta': delta,\n",
    "            'delta_pct': delta_pct,\n",
    "            'threshold_pct': threshold_pct,\n",
    "            'reward': reward,\n",
    "            'action': action\n",
    "        }\n",
    "\n",
    "        self.current_step += 1\n",
    "        terminated = self.current_step >= len(self.data) - 1\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._get_obs() if not terminated else np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return self.data.iloc[self.current_step].values\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Evaluation function\n",
    "# =========================\n",
    "def evaluate_model(model, vec_env):\n",
    "    obs = vec_env.reset()\n",
    "    terminated = False\n",
    "    total_steps = 0\n",
    "    num_correct = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    print(\"Starting evaluation episode\")\n",
    "    while not terminated:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, term, trunc, infos = vec_env.step(action)\n",
    "        terminated = term[0] or trunc[0]\n",
    "\n",
    "        info = infos[0]\n",
    "        reward = rewards[0]\n",
    "        delta_pct = info['delta_pct'] * 100\n",
    "        step = info['step']\n",
    "        action_val = action[0]\n",
    "\n",
    "        action_str = ['Hold', 'Buy', 'Sell'][action_val]\n",
    "        if abs(delta_pct) < info['threshold_pct'] * 100:\n",
    "            movement = 'stable'\n",
    "        elif delta_pct > 0:\n",
    "            movement = 'up'\n",
    "        else:\n",
    "            movement = 'down'\n",
    "\n",
    "        print(f\"Step {step}, chosen action {action_str}, next price movement {movement} ({delta_pct:.2f}%), reward {reward:.4f}\")\n",
    "\n",
    "        total_steps += 1\n",
    "        total_reward += reward\n",
    "        if reward >= 0:\n",
    "            num_correct += 1\n",
    "\n",
    "    accuracy = (num_correct / total_steps) * 100 if total_steps > 0 else 0\n",
    "    print(\"Evaluation completed.\")\n",
    "    print(f\"Final evaluation accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Total cumulative reward: {total_reward:.4f}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('BTC-USD_with_Indicators.csv')\n",
    "    env = BTCTradingEnv(df)\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    model_path = 'btc_dqn_agent.zip'\n",
    "    trained = False\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        model = DQN.load(model_path, env=vec_env)\n",
    "        print(\"Loaded existing model.\")\n",
    "    else:\n",
    "        model = DQN(\"MlpPolicy\", vec_env, verbose=1)\n",
    "        callback = CustomCallback()\n",
    "\n",
    "        steps_per_episode = len(df) - 1\n",
    "        total_timesteps = 5 * steps_per_episode\n",
    "\n",
    "        model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "        model.save(model_path)\n",
    "        print(\"Trained and saved new model.\")\n",
    "        trained = True\n",
    "\n",
    "        plt.plot(callback.episode_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Training Reward History')\n",
    "        plt.show()\n",
    "\n",
    "    evaluate_model(model, vec_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1fe672",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### observation of 20  past lines \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# =========================\n",
    "# Custom callback for printing during training\n",
    "# =========================\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.episode = 0\n",
    "        self.num_correct = 0\n",
    "        self.total_steps = 0\n",
    "        self.current_ep_reward = 0\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        info = self.locals['infos'][0]\n",
    "        action = self.locals['actions'][0]\n",
    "        reward = self.locals['rewards'][0]\n",
    "        done = self.locals['dones'][0]\n",
    "\n",
    "        step = info['step']\n",
    "        delta_pct = info['delta_pct'] * 100  # convert to % for readability\n",
    "        action_str = ['Hold', 'Buy', 'Sell'][action]\n",
    "\n",
    "        if abs(delta_pct) < info['threshold_pct'] * 100:\n",
    "            movement = 'stable'\n",
    "        elif delta_pct > 0:\n",
    "            movement = 'up'\n",
    "        else:\n",
    "            movement = 'down'\n",
    "        movement_str = f\"{movement} ({delta_pct:.2f}%)\"\n",
    "\n",
    "        print(f\"Step {step}, chosen action {action_str}, next price movement {movement_str}, reward {reward:.4f}\")\n",
    "\n",
    "        # Track accuracy (reward >= 0 considered correct)\n",
    "        self.total_steps += 1\n",
    "        if reward >= 0:\n",
    "            self.num_correct += 1\n",
    "        accuracy = (self.num_correct / self.total_steps) * 100\n",
    "        print(f\"Running accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        self.current_ep_reward += reward\n",
    "\n",
    "        if step == 0:\n",
    "            print(f\"Episode {self.episode + 1}\")\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode {self.episode + 1} completed.\")\n",
    "            self.episode_rewards.append(self.current_ep_reward)\n",
    "            self.current_ep_reward = 0\n",
    "            self.episode += 1\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Custom BTC Trading Environment\n",
    "# =========================\n",
    "class BTCTradingEnv(gym.Env):\n",
    "    def __init__(self, data, lookback_window=20, transaction_cost=0.001):\n",
    "        super(BTCTradingEnv, self).__init__()\n",
    "        self.data = data.drop(columns=['OpenTime'])\n",
    "        self.data = self.data.astype(np.float32)\n",
    "\n",
    "        self.lookback = lookback_window\n",
    "        self.transaction_cost = transaction_cost\n",
    "\n",
    "        obs_shape = (self.lookback * self.data.shape[1],)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=obs_shape, dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(3)  # Hold, Buy, Sell\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.current_step = self.lookback  # start after enough history\n",
    "        obs = self._get_obs()\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        current_close = self.data.iloc[self.current_step]['Close']\n",
    "        next_close = self.data.iloc[self.current_step + 1]['Close']\n",
    "        delta = next_close - current_close\n",
    "        delta_pct = delta / current_close\n",
    "\n",
    "        # Threshold for \"stable\"\n",
    "        threshold_pct = 0.001\n",
    "\n",
    "        # Reward system\n",
    "        if action == 0:  # Hold\n",
    "            if abs(delta_pct) < threshold_pct:\n",
    "                reward = 0.05\n",
    "            else:\n",
    "                reward = -abs(delta_pct)\n",
    "        elif action == 1:  # Buy\n",
    "            reward = delta_pct - self.transaction_cost\n",
    "        elif action == 2:  # Sell\n",
    "            reward = -delta_pct - self.transaction_cost\n",
    "\n",
    "        info = {\n",
    "            'step': self.current_step,\n",
    "            'delta': delta,\n",
    "            'delta_pct': delta_pct,\n",
    "            'threshold_pct': threshold_pct,\n",
    "            'reward': reward,\n",
    "            'action': action\n",
    "        }\n",
    "\n",
    "        self.current_step += 1\n",
    "        terminated = self.current_step >= len(self.data) - 1\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._get_obs() if not terminated else np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        start = self.current_step - self.lookback\n",
    "        end = self.current_step\n",
    "        window = self.data.iloc[start:end].values\n",
    "        return window.flatten().astype(np.float32)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Evaluation function\n",
    "# =========================\n",
    "def evaluate_model(model, vec_env):\n",
    "    obs = vec_env.reset()\n",
    "    terminated = False\n",
    "    total_steps = 0\n",
    "    num_correct = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    print(\"Starting evaluation episode\")\n",
    "    while not terminated:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, term, trunc, infos = vec_env.step(action)\n",
    "        terminated = term[0] or trunc[0]\n",
    "\n",
    "        info = infos[0]\n",
    "        reward = rewards[0]\n",
    "        delta_pct = info['delta_pct'] * 100\n",
    "        step = info['step']\n",
    "        action_val = action[0]\n",
    "\n",
    "        action_str = ['Hold', 'Buy', 'Sell'][action_val]\n",
    "        if abs(delta_pct) < info['threshold_pct'] * 100:\n",
    "            movement = 'stable'\n",
    "        elif delta_pct > 0:\n",
    "            movement = 'up'\n",
    "        else:\n",
    "            movement = 'down'\n",
    "\n",
    "        print(f\"Step {step}, chosen action {action_str}, next price movement {movement} ({delta_pct:.2f}%), reward {reward:.4f}\")\n",
    "\n",
    "        total_steps += 1\n",
    "        total_reward += reward\n",
    "        if reward >= 0:\n",
    "            num_correct += 1\n",
    "\n",
    "    accuracy = (num_correct / total_steps) * 100 if total_steps > 0 else 0\n",
    "    print(\"Evaluation completed.\")\n",
    "    print(f\"Final evaluation accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Total cumulative reward: {total_reward:.4f}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('BTC-USD_with_Indicators.csv')\n",
    "    env = BTCTradingEnv(df, lookback_window=20, transaction_cost=0.001)\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    model_path = 'btc_dqn_agent.zip'\n",
    "    trained = False\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        model = DQN.load(model_path, env=vec_env)\n",
    "        print(\"Loaded existing model.\")\n",
    "    else:\n",
    "        model = DQN(\"MlpPolicy\", vec_env, verbose=1)\n",
    "        callback = CustomCallback()\n",
    "\n",
    "        steps_per_episode = len(df) - 1\n",
    "        total_timesteps = 5 * steps_per_episode\n",
    "\n",
    "        model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "        model.save(model_path)\n",
    "        print(\"Trained and saved new model.\")\n",
    "        trained = True\n",
    "\n",
    "        plt.plot(callback.episode_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Training Reward History')\n",
    "        plt.show()\n",
    "\n",
    "    evaluate_model(model, vec_env)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
