{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59aee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# RL Cheat Sheet - Stable-Baselines3\n",
    "# ===============================\n",
    "\n",
    "# 1. Import Libraries\n",
    "import gymnasium as gym                  # Gym environments\n",
    "from stable_baselines3 import PPO, DQN, SAC\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# ===============================\n",
    "# 2. Create Environment\n",
    "# ===============================\n",
    "# Standard environment\n",
    "env = gym.make(\"CartPole-v1\")           # Discrete action environment\n",
    "# Continuous action environment (for SAC)\n",
    "# env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# Custom Environment Example\n",
    "\"\"\"\n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(n_features,))\n",
    "        self.action_space = gym.spaces.Discrete(n_actions) # or Box for continuous\n",
    "    def reset(self):\n",
    "        return self.observation_space.sample(), {}\n",
    "    def step(self, action):\n",
    "        obs = self.observation_space.sample()\n",
    "        reward = ...  # define reward\n",
    "        done = ...    # define episode end\n",
    "        return obs, reward, done, False, {}\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# 3. Initialize Model\n",
    "# ===============================\n",
    "# PPO (on-policy, discrete/continuous)\n",
    "ppo_model = PPO(\n",
    "    \"MlpPolicy\", env,\n",
    "    learning_rate=3e-4,       # learning rate for policy network\n",
    "    n_steps=128,              # steps per rollout\n",
    "    batch_size=64,            # mini-batch size\n",
    "    gamma=0.99,               # discount factor\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./ppo_logs/\"\n",
    ")\n",
    "\n",
    "# DQN (off-policy, discrete only)\n",
    "dqn_model = DQN(\n",
    "    \"MlpPolicy\", env,\n",
    "    learning_rate=1e-3,       # Q-network learning rate\n",
    "    buffer_size=50000,        # replay buffer size\n",
    "    learning_starts=1000,     # steps before training starts\n",
    "    batch_size=32,            # mini-batch size\n",
    "    gamma=0.99,               # discount factor\n",
    "    target_update_interval=500,  # update target network\n",
    "    exploration_fraction=0.1,    # fraction of training for epsilon decay\n",
    "    exploration_final_eps=0.05,  # final epsilon value\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# SAC (off-policy, continuous only)\n",
    "# Note: SAC requires Box (continuous) action space\n",
    "# sac_model = SAC(\n",
    "#     \"MlpPolicy\", env,\n",
    "#     learning_rate=3e-4,\n",
    "#     buffer_size=50000,\n",
    "#     batch_size=64,\n",
    "#     gamma=0.99,\n",
    "#     tau=0.005,  # soft update for target network\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# ===============================\n",
    "# 4. Train Model\n",
    "# ===============================\n",
    "# total_timesteps = number of environment steps\n",
    "ppo_model.learn(total_timesteps=50000)\n",
    "dqn_model.learn(total_timesteps=50000)\n",
    "# sac_model.learn(total_timesteps=50000)\n",
    "\n",
    "# ===============================\n",
    "# 5. Evaluate Model\n",
    "# ===============================\n",
    "# Evaluate over n episodes\n",
    "mean_reward, std_reward = evaluate_policy(ppo_model, env, n_eval_episodes=10)\n",
    "# same for dqn_model, sac_model\n",
    "print(f\"PPO mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# ===============================\n",
    "# 6. Test / Run Agent\n",
    "# ===============================\n",
    "obs, _ = env.reset()\n",
    "for _ in range(100):\n",
    "    # Deterministic actions (choose best)\n",
    "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {reward}\")\n",
    "    if done or truncated:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "# ===============================\n",
    "# 7. Save and Load Model\n",
    "# ===============================\n",
    "ppo_model.save(\"ppo_model\")               # save model\n",
    "loaded_model = PPO.load(\"ppo_model\", env=env)  # load model\n",
    "\n",
    "# ===============================\n",
    "# 8. Important Parameters Summary\n",
    "# ===============================\n",
    "\"\"\"\n",
    "Common RL Parameters:\n",
    "\n",
    "- learning_rate: how fast the agent updates NN weights\n",
    "- gamma: discount factor for future rewards\n",
    "- n_steps: PPO rollout steps\n",
    "- batch_size: mini-batch size for training\n",
    "- buffer_size: replay buffer size (DQN/SAC)\n",
    "- learning_starts: steps before training begins (DQN/SAC)\n",
    "- target_update_interval: how often to update target network (DQN)\n",
    "- tau: soft update rate (SAC)\n",
    "- exploration_fraction: fraction of training for epsilon decay (DQN)\n",
    "- exploration_final_eps: final epsilon (DQN)\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# 9. Exploration / Strategy\n",
    "# ===============================\n",
    "\"\"\"\n",
    "- PPO: stochastic policy, exploration intrinsic\n",
    "- DQN: epsilon-greedy\n",
    "- SAC: stochastic policy with entropy bonus (encourages exploration)\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# 10. Notes\n",
    "# ===============================\n",
    "\"\"\"\n",
    "- PPO: stable, works for discrete & continuous actions\n",
    "- DQN: discrete actions only, uses replay buffer & target network\n",
    "- SAC: continuous actions, off-policy, sample efficient, uses soft target updates\n",
    "- Always normalize / preprocess observations for stable learning\n",
    "- Use small total_timesteps initially for testing\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "                          +-----------------+\n",
    "                          |   Environment   |\n",
    "                          |   (Gym/Custom)  |\n",
    "                          +--------+--------+\n",
    "                                   |\n",
    "          ------------------------------------------------\n",
    "          |                       |                     |\n",
    "       [PPO]                   [DQN]                  [SAC]\n",
    "          |                       |                     |\n",
    "  +-------+--------+      +-------+--------+    +-------+--------+\n",
    "  | Policy Network |      | Q-Network      |    | Actor Network  |\n",
    "  | (MLP/CNN)     |      | (MLP/CNN)      |    | (MLP/CNN)      |\n",
    "  +-------+--------+      +-------+--------+    +-------+--------+\n",
    "          |                       |                     |\n",
    "          |                       |                     |\n",
    "      Action Sampling         Îµ-greedy / Policy       Stochastic Policy\n",
    "      (stochastic)            from Q-values           + Entropy Bonus\n",
    "          |                       |                     |\n",
    "          +-----------+-----------+-----------+---------+\n",
    "                      |\n",
    "                   Step in Environment\n",
    "                      |\n",
    "              +-------+--------+\n",
    "              | Observe Next   |\n",
    "              | State & Reward |\n",
    "              +-------+--------+\n",
    "                      |\n",
    "      ------------------------------------------\n",
    "      |                       |                |\n",
    "    PPO: Rollout           DQN: Store in     SAC: Store in\n",
    "    n_steps trajectories   Replay Buffer      Replay Buffer\n",
    "      |                       |                |\n",
    "   Compute Advantage     Sample Mini-Batch   Sample Mini-Batch\n",
    "      |                       |                |\n",
    "   Policy Gradient       Update Q-network   Update Actor + Critic\n",
    "   Loss & Backprop       Loss & Backprop   Loss & Backprop\n",
    "      |                       |                |\n",
    "      +-----------+-----------+----------------+\n",
    "                  |\n",
    "           Update Networks\n",
    "       (Target network for DQN/SAC)\n",
    "                  |\n",
    "           Repeat Until Done\n",
    "                  |\n",
    "           Evaluate / Save / Test\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
