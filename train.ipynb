{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a623923c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "####### Improved reward system with proper structure\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# =========================\n",
    "# Constants\n",
    "# =========================\n",
    "STABLE_THRESHOLD = 0.001  # 0.1% movement considered stable\n",
    "# =========================\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.episode = 0\n",
    "        self.num_correct = 0\n",
    "        self.total_steps = 0\n",
    "        self.current_ep_reward = 0\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        info = self.locals['infos'][0]\n",
    "        action = self.locals['actions'][0]\n",
    "        reward = self.locals['rewards'][0]\n",
    "        done = self.locals['dones'][0]\n",
    "\n",
    "        step = info['step']\n",
    "        delta_pct = info['delta_pct'] * 100  # convert to % for readability\n",
    "        action_str = ['Hold', 'Buy', 'Sell'][action]\n",
    "\n",
    "        if abs(delta_pct) < info['threshold_pct'] * 100:\n",
    "            movement = 'stable'\n",
    "        elif delta_pct > 0:\n",
    "            movement = 'up'\n",
    "        else:\n",
    "            movement = 'down'\n",
    "        movement_str = f\"{movement} ({delta_pct:.2f}%)\"\n",
    "\n",
    "        print(f\"Step {step}, chosen action {action_str}, next price movement {movement_str}, reward {reward:.4f}\")\n",
    "\n",
    "        # Track accuracy (reward >= 0 considered correct)\n",
    "        self.total_steps += 1\n",
    "        if reward >= 0:\n",
    "            self.num_correct += 1\n",
    "        accuracy = (self.num_correct / self.total_steps) * 100\n",
    "        print(f\"Running accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        self.current_ep_reward += reward\n",
    "\n",
    "        if step == 0:\n",
    "            print(f\"Episode {self.episode + 1}\")\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode {self.episode + 1} completed.\")\n",
    "            self.episode_rewards.append(self.current_ep_reward)\n",
    "            self.current_ep_reward = 0\n",
    "            self.episode += 1\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Custom BTC Trading Environment\n",
    "# =========================\n",
    "class BTCTradingEnv(gym.Env):\n",
    "    def __init__(self, data, lookback_window=12, transaction_cost=0.001):\n",
    "        super(BTCTradingEnv, self).__init__()\n",
    "        self.data = data.drop(columns=['OpenTime'])\n",
    "        self.data = self.data.astype(np.float32)\n",
    "\n",
    "        self.lookback = lookback_window\n",
    "        self.transaction_cost = transaction_cost\n",
    "\n",
    "        obs_shape = (self.lookback * self.data.shape[1],)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=obs_shape, dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(3)  # Hold, Buy, Sell\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.current_step = self.lookback  # start after enough history\n",
    "        obs = self._get_obs()\n",
    "        return obs, {}\n",
    "\n",
    "    def _calculate_reward(self, action, delta_pct):\n",
    "    # Define thresholds\n",
    "        up_threshold = 0.001   # >0.1% = up\n",
    "        down_threshold = -0.001 # <-0.1% = down\n",
    "\n",
    "        reward = 0.0\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            if delta_pct > up_threshold:\n",
    "                reward = 1 + abs(delta_pct)  # correct, stronger move = bigger reward\n",
    "            else:\n",
    "                reward = -1                  # wrong prediction\n",
    "\n",
    "        elif action == 2:  # Sell\n",
    "            if delta_pct < down_threshold:\n",
    "                reward = 1 + abs(delta_pct)  # correct, stronger down = bigger reward\n",
    "            else:\n",
    "                reward = -1\n",
    "\n",
    "        else:  # Hold\n",
    "            if abs(delta_pct) <= up_threshold:   # stable market\n",
    "                reward = 0.5                     # small positive reward\n",
    "            else:\n",
    "                reward = -0.5                    # should have predicted moves\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        current_close = self.data.iloc[self.current_step]['Close']\n",
    "        next_close = self.data.iloc[self.current_step + 1]['Close']\n",
    "        delta = next_close - current_close\n",
    "        delta_pct = delta / current_close\n",
    "\n",
    "        # Calculate reward using improved system\n",
    "        reward = self._calculate_reward(action, delta_pct)\n",
    "\n",
    "        info = {\n",
    "            'step': self.current_step,\n",
    "            'delta': delta,\n",
    "            'delta_pct': delta_pct,\n",
    "            'threshold_pct': STABLE_THRESHOLD,\n",
    "            'reward': reward,\n",
    "            'action': action,\n",
    "        }\n",
    "\n",
    "        self.current_step += 1\n",
    "        terminated = self.current_step >= len(self.data) - 1\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._get_obs() if not terminated else np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        start = self.current_step - self.lookback\n",
    "        end = self.current_step\n",
    "        window = self.data.iloc[start:end].values\n",
    "        return window.flatten().astype(np.float32)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Evaluation function\n",
    "# =========================\n",
    "def evaluate_model(model, vec_env):\n",
    "    obs = vec_env.reset()\n",
    "    terminated = False\n",
    "    total_steps = 0\n",
    "    num_correct = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    print(\"Starting evaluation episode\")\n",
    "    while not terminated:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, term, trunc, infos = vec_env.step(action)\n",
    "        terminated = term[0] or trunc[0]\n",
    "\n",
    "        info = infos[0]\n",
    "        reward = rewards[0]\n",
    "        delta_pct = info['delta_pct'] * 100\n",
    "        step = info['step']\n",
    "        action_val = action[0]\n",
    "\n",
    "        action_str = ['Hold', 'Buy', 'Sell'][action_val]\n",
    "        if abs(delta_pct) < info['threshold_pct'] * 100:\n",
    "            movement = 'stable'\n",
    "        elif delta_pct > 0:\n",
    "            movement = 'up'\n",
    "        else:\n",
    "            movement = 'down'\n",
    "\n",
    "        print(f\"Step {step}, chosen action {action_str}, next price movement {movement} ({delta_pct:.2f}%), reward {reward:.4f}\")\n",
    "\n",
    "        total_steps += 1\n",
    "        total_reward += reward\n",
    "        if reward >= 0:\n",
    "            num_correct += 1\n",
    "\n",
    "    accuracy = (num_correct / total_steps) * 100 if total_steps > 0 else 0\n",
    "    print(\"Evaluation completed.\")\n",
    "    print(f\"Final evaluation accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Total cumulative reward: {total_reward:.4f}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('BTC-USD_with_Indicators.csv')\n",
    "    env = BTCTradingEnv(df, lookback_window=12, transaction_cost=0.001)\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    model_path = 'btc_dqn_agent.zip'\n",
    "    trained = False\n",
    "    if os.path.exists(model_path):\n",
    "        model = DQN.load(model_path, env=vec_env)\n",
    "        print(\"Loaded existing model.\")\n",
    "    else:\n",
    "        # model = DQN(\"MlpPolicy\", vec_env, verbose=1)\n",
    "        model = DQN(\n",
    "            \"MlpPolicy\",\n",
    "            vec_env,\n",
    "            verbose=1,\n",
    "            learning_rate=1e-4,\n",
    "            buffer_size=50000,\n",
    "            learning_starts=1000,\n",
    "            batch_size=32,\n",
    "            tau=0.05,\n",
    "            gamma=0.99,\n",
    "            train_freq=4,\n",
    "            target_update_interval=500,\n",
    "            exploration_fraction=0.2,\n",
    "            exploration_final_eps=0.05,\n",
    "        )\n",
    "        callback = CustomCallback()\n",
    "\n",
    "        steps_per_episode = len(df) - 1\n",
    "        total_timesteps = 20 * steps_per_episode\n",
    "\n",
    "        model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "        model.save(model_path)\n",
    "        print(\"Trained and saved new model.\")\n",
    "        trained = True\n",
    "\n",
    "        plt.plot(callback.episode_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Training Reward History')\n",
    "        plt.show()\n",
    "\n",
    "    evaluate_model(model, vec_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f51632c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env 0 Episode 1 reward: -25.24\n",
      "Env 1 Episode 1 reward: -54.71\n",
      "Env 2 Episode 1 reward: -41.74\n",
      "Env 3 Episode 1 reward: -50.34\n",
      "Env 4 Episode 1 reward: -32.73\n",
      "Env 5 Episode 1 reward: -45.82\n",
      "Env 6 Episode 1 reward: -71.40\n",
      "Env 7 Episode 1 reward: -37.32\n",
      "Env 0 Episode 2 reward: -73.37\n",
      "Env 1 Episode 2 reward: -43.82\n",
      "Env 2 Episode 2 reward: -72.89\n",
      "Env 3 Episode 2 reward: -61.38\n",
      "Env 4 Episode 2 reward: -55.83\n",
      "Env 5 Episode 2 reward: -57.29\n",
      "Env 6 Episode 2 reward: -72.85\n",
      "Env 7 Episode 2 reward: -39.73\n",
      "Env 0 Episode 3 reward: -61.81\n",
      "Env 1 Episode 3 reward: -63.39\n",
      "Env 2 Episode 3 reward: -34.32\n",
      "Env 3 Episode 3 reward: -48.84\n",
      "Env 4 Episode 3 reward: -39.18\n",
      "Env 5 Episode 3 reward: -57.30\n",
      "Env 6 Episode 3 reward: -60.42\n",
      "Env 7 Episode 3 reward: -61.38\n",
      "Env 0 Episode 4 reward: -72.93\n",
      "Env 1 Episode 4 reward: -32.26\n",
      "Env 2 Episode 4 reward: -34.48\n",
      "Env 3 Episode 4 reward: -40.28\n",
      "Env 4 Episode 4 reward: -62.87\n",
      "Env 5 Episode 4 reward: -51.77\n",
      "Env 6 Episode 4 reward: -36.16\n",
      "Env 7 Episode 4 reward: -55.28\n",
      "Env 0 Episode 5 reward: -29.33\n",
      "Env 1 Episode 5 reward: -47.73\n",
      "Env 2 Episode 5 reward: -38.63\n",
      "Env 3 Episode 5 reward: -58.74\n",
      "Env 4 Episode 5 reward: -58.27\n",
      "Env 5 Episode 5 reward: -45.31\n",
      "Env 6 Episode 5 reward: -45.81\n",
      "Env 7 Episode 5 reward: -25.76\n",
      "Env 0 Episode 6 reward: -64.81\n",
      "Env 1 Episode 6 reward: -59.36\n",
      "Env 2 Episode 6 reward: -56.85\n",
      "Env 3 Episode 6 reward: -75.33\n",
      "Env 4 Episode 6 reward: -38.05\n",
      "Env 5 Episode 6 reward: -63.33\n",
      "Env 6 Episode 6 reward: -33.72\n",
      "Env 7 Episode 6 reward: -71.87\n",
      "Env 0 Episode 7 reward: -28.16\n",
      "Env 1 Episode 7 reward: -75.39\n",
      "Env 2 Episode 7 reward: -44.85\n",
      "Env 3 Episode 7 reward: -48.86\n",
      "Env 4 Episode 7 reward: -44.26\n",
      "Env 5 Episode 7 reward: -53.84\n",
      "Env 6 Episode 7 reward: -43.18\n",
      "Env 7 Episode 7 reward: -64.38\n",
      "Env 0 Episode 8 reward: -53.76\n",
      "Env 1 Episode 8 reward: -43.29\n",
      "Env 2 Episode 8 reward: -48.83\n",
      "Env 3 Episode 8 reward: -43.31\n",
      "Env 4 Episode 8 reward: -58.32\n",
      "Env 5 Episode 8 reward: -70.32\n",
      "Env 6 Episode 8 reward: -66.38\n",
      "Env 7 Episode 8 reward: -44.32\n",
      "Env 0 Episode 9 reward: -52.26\n",
      "Env 1 Episode 9 reward: -54.94\n",
      "Env 2 Episode 9 reward: -42.20\n",
      "Env 3 Episode 9 reward: -58.27\n",
      "Env 4 Episode 9 reward: -48.29\n",
      "Env 5 Episode 9 reward: -62.50\n",
      "Env 6 Episode 9 reward: -44.27\n",
      "Env 7 Episode 9 reward: -53.78\n",
      "Env 0 Episode 10 reward: -55.75\n",
      "Env 1 Episode 10 reward: -58.84\n",
      "Env 2 Episode 10 reward: -45.24\n",
      "Env 3 Episode 10 reward: -59.67\n",
      "Env 4 Episode 10 reward: -60.75\n",
      "Env 5 Episode 10 reward: -42.18\n",
      "Env 6 Episode 10 reward: -65.33\n",
      "Env 7 Episode 10 reward: -59.23\n",
      "Env 0 Episode 11 reward: -46.33\n",
      "Env 1 Episode 11 reward: -54.37\n",
      "Env 2 Episode 11 reward: -35.11\n",
      "Env 3 Episode 11 reward: -56.28\n",
      "Env 4 Episode 11 reward: -24.37\n",
      "Env 5 Episode 11 reward: -60.85\n",
      "Env 6 Episode 11 reward: -47.57\n",
      "Env 7 Episode 11 reward: -46.88\n",
      "Env 0 Episode 12 reward: -41.67\n",
      "Env 1 Episode 12 reward: -49.78\n",
      "Env 2 Episode 12 reward: -41.27\n",
      "Env 3 Episode 12 reward: -24.53\n",
      "Env 4 Episode 12 reward: -42.25\n",
      "Env 5 Episode 12 reward: -39.78\n",
      "Env 6 Episode 12 reward: -44.32\n",
      "Env 7 Episode 12 reward: -53.85\n",
      "Env 0 Episode 13 reward: -47.72\n",
      "Env 1 Episode 13 reward: -47.74\n",
      "Env 2 Episode 13 reward: -31.10\n",
      "Env 3 Episode 13 reward: -45.24\n",
      "Env 4 Episode 13 reward: -30.73\n",
      "Env 5 Episode 13 reward: -58.90\n",
      "Env 6 Episode 13 reward: -36.12\n",
      "Env 7 Episode 13 reward: -46.23\n",
      "Env 0 Episode 14 reward: -56.75\n",
      "Env 1 Episode 14 reward: -51.30\n",
      "Env 2 Episode 14 reward: -47.76\n",
      "Env 3 Episode 14 reward: -43.80\n",
      "Env 4 Episode 14 reward: -62.86\n",
      "Env 5 Episode 14 reward: -39.18\n",
      "Env 6 Episode 14 reward: -56.85\n",
      "Env 7 Episode 14 reward: -67.32\n",
      "Env 0 Episode 15 reward: -28.12\n",
      "Env 1 Episode 15 reward: -31.54\n",
      "Env 2 Episode 15 reward: -46.60\n",
      "Env 3 Episode 15 reward: -68.66\n",
      "Env 4 Episode 15 reward: -63.81\n",
      "Env 5 Episode 15 reward: -46.19\n",
      "Env 6 Episode 15 reward: -41.84\n",
      "Env 7 Episode 15 reward: -55.79\n",
      "Env 0 Episode 16 reward: -80.39\n",
      "Env 1 Episode 16 reward: -52.36\n",
      "Env 2 Episode 16 reward: -31.95\n",
      "Env 3 Episode 16 reward: -50.07\n",
      "Env 4 Episode 16 reward: -50.68\n",
      "Env 5 Episode 16 reward: -50.64\n",
      "Env 6 Episode 16 reward: -81.38\n",
      "Env 7 Episode 16 reward: -16.80\n",
      "Env 0 Episode 17 reward: -33.28\n",
      "Env 1 Episode 17 reward: -52.75\n",
      "Env 2 Episode 17 reward: -57.43\n",
      "Env 3 Episode 17 reward: -26.55\n",
      "Env 4 Episode 17 reward: -57.38\n",
      "Env 5 Episode 17 reward: -35.92\n",
      "Env 6 Episode 17 reward: -35.74\n",
      "Env 7 Episode 17 reward: -63.87\n",
      "Env 0 Episode 18 reward: -61.36\n",
      "Env 1 Episode 18 reward: -44.18\n",
      "Env 2 Episode 18 reward: -66.75\n",
      "Env 3 Episode 18 reward: -43.24\n",
      "Env 4 Episode 18 reward: -46.80\n",
      "Env 5 Episode 18 reward: -53.40\n",
      "Env 6 Episode 18 reward: -77.41\n",
      "Env 7 Episode 18 reward: -55.35\n",
      "Env 0 Episode 19 reward: -20.67\n",
      "Env 1 Episode 19 reward: -32.67\n",
      "Env 2 Episode 19 reward: -52.27\n",
      "Env 3 Episode 19 reward: -31.94\n",
      "Env 4 Episode 19 reward: -45.25\n",
      "Env 5 Episode 19 reward: -30.74\n",
      "Env 6 Episode 19 reward: -50.26\n",
      "Env 7 Episode 19 reward: -60.72\n",
      "Env 0 Episode 20 reward: -57.82\n",
      "Env 1 Episode 20 reward: -64.33\n",
      "Env 2 Episode 20 reward: -44.25\n",
      "Env 3 Episode 20 reward: -54.26\n",
      "Env 4 Episode 20 reward: -40.30\n",
      "Env 5 Episode 20 reward: -56.81\n",
      "Env 6 Episode 20 reward: -80.94\n",
      "Env 7 Episode 20 reward: -56.78\n",
      "Env 0 Episode 21 reward: -41.74\n",
      "Env 1 Episode 21 reward: -44.31\n",
      "Env 2 Episode 21 reward: -58.81\n",
      "Env 3 Episode 21 reward: -57.51\n",
      "Env 4 Episode 21 reward: -49.75\n",
      "Env 5 Episode 21 reward: -27.21\n",
      "Env 6 Episode 21 reward: -60.34\n",
      "Env 7 Episode 21 reward: -47.75\n",
      "Env 0 Episode 22 reward: -68.19\n",
      "Env 1 Episode 22 reward: -53.89\n",
      "Env 2 Episode 22 reward: -55.83\n",
      "Env 3 Episode 22 reward: -47.34\n",
      "Env 4 Episode 22 reward: -32.80\n",
      "Env 5 Episode 22 reward: -51.79\n",
      "Env 6 Episode 22 reward: -39.80\n",
      "Env 7 Episode 22 reward: -45.69\n",
      "Env 0 Episode 23 reward: -24.69\n",
      "Env 1 Episode 23 reward: -51.35\n",
      "Env 2 Episode 23 reward: -15.70\n",
      "Env 3 Episode 23 reward: -56.64\n",
      "Env 4 Episode 23 reward: -39.70\n",
      "Env 5 Episode 23 reward: -31.18\n",
      "Env 6 Episode 23 reward: -55.77\n",
      "Env 7 Episode 23 reward: -57.58\n",
      "Env 0 Episode 24 reward: -63.31\n",
      "Env 1 Episode 24 reward: -54.33\n",
      "Env 2 Episode 24 reward: -53.76\n",
      "Env 3 Episode 24 reward: -33.09\n",
      "Env 4 Episode 24 reward: -30.83\n",
      "Env 5 Episode 24 reward: -70.26\n",
      "Env 6 Episode 24 reward: -71.42\n",
      "Env 7 Episode 24 reward: -51.75\n",
      "Env 0 Episode 25 reward: -59.27\n",
      "Env 1 Episode 25 reward: -47.63\n",
      "Env 2 Episode 25 reward: -47.77\n",
      "Env 3 Episode 25 reward: -59.36\n",
      "Env 4 Episode 25 reward: -76.91\n",
      "Env 5 Episode 25 reward: -48.32\n",
      "Env 6 Episode 25 reward: -45.71\n",
      "Env 7 Episode 25 reward: -31.28\n",
      "Env 0 Episode 26 reward: -46.34\n",
      "Env 1 Episode 26 reward: -39.62\n",
      "Env 2 Episode 26 reward: -44.77\n",
      "Env 3 Episode 26 reward: -48.79\n",
      "Env 4 Episode 26 reward: -77.89\n",
      "Env 5 Episode 26 reward: -42.77\n",
      "Env 6 Episode 26 reward: -39.69\n",
      "Env 7 Episode 26 reward: -42.21\n",
      "Env 0 Episode 27 reward: -20.04\n",
      "Env 1 Episode 27 reward: -51.30\n",
      "Env 2 Episode 27 reward: -46.75\n",
      "Env 3 Episode 27 reward: -47.80\n",
      "Env 4 Episode 27 reward: -30.51\n",
      "Env 5 Episode 27 reward: -49.11\n",
      "Env 6 Episode 27 reward: -35.14\n",
      "Env 7 Episode 27 reward: -42.18\n",
      "Env 0 Episode 28 reward: -49.79\n",
      "Env 1 Episode 28 reward: -42.84\n",
      "Env 2 Episode 28 reward: -69.38\n",
      "Env 3 Episode 28 reward: -66.39\n",
      "Env 4 Episode 28 reward: -43.27\n",
      "Env 5 Episode 28 reward: -39.84\n",
      "Env 6 Episode 28 reward: -43.32\n",
      "Env 7 Episode 28 reward: -62.87\n",
      "Env 0 Episode 29 reward: -49.77\n",
      "Env 1 Episode 29 reward: -31.65\n",
      "Env 2 Episode 29 reward: -64.75\n",
      "Env 3 Episode 29 reward: -74.37\n",
      "Env 4 Episode 29 reward: -56.86\n",
      "Env 5 Episode 29 reward: -48.13\n",
      "Env 6 Episode 29 reward: -48.18\n",
      "Env 7 Episode 29 reward: -62.76\n",
      "Env 0 Episode 30 reward: -72.40\n",
      "Env 1 Episode 30 reward: -46.28\n",
      "Env 2 Episode 30 reward: -60.85\n",
      "Env 3 Episode 30 reward: -76.80\n",
      "Env 4 Episode 30 reward: -58.71\n",
      "Env 5 Episode 30 reward: -74.38\n",
      "Env 6 Episode 30 reward: -77.87\n",
      "Env 7 Episode 30 reward: -26.57\n",
      "Env 0 Episode 31 reward: -69.83\n",
      "Env 1 Episode 31 reward: -37.77\n",
      "Env 2 Episode 31 reward: -40.64\n",
      "Env 3 Episode 31 reward: -60.28\n",
      "Env 4 Episode 31 reward: -31.29\n",
      "Env 5 Episode 31 reward: -49.82\n",
      "Env 6 Episode 31 reward: -43.08\n",
      "Env 7 Episode 31 reward: -64.85\n",
      "Env 0 Episode 32 reward: -57.34\n",
      "Env 1 Episode 32 reward: -52.15\n",
      "Env 2 Episode 32 reward: -34.29\n",
      "Env 3 Episode 32 reward: -17.65\n",
      "Env 4 Episode 32 reward: -32.69\n",
      "Env 5 Episode 32 reward: -76.94\n",
      "Env 6 Episode 32 reward: -37.99\n",
      "Env 7 Episode 32 reward: -70.32\n",
      "Env 0 Episode 33 reward: -52.35\n",
      "Env 1 Episode 33 reward: -48.81\n",
      "Env 2 Episode 33 reward: -40.69\n",
      "Env 3 Episode 33 reward: -40.88\n",
      "Env 4 Episode 33 reward: -40.58\n",
      "Env 5 Episode 33 reward: -50.30\n",
      "Env 6 Episode 33 reward: -42.28\n",
      "Env 7 Episode 33 reward: -37.27\n",
      "Env 0 Episode 34 reward: -50.69\n",
      "Env 1 Episode 34 reward: -59.30\n",
      "Env 2 Episode 34 reward: -50.49\n",
      "Env 3 Episode 34 reward: -40.77\n",
      "Env 4 Episode 34 reward: -30.72\n",
      "Env 5 Episode 34 reward: -46.50\n",
      "Env 6 Episode 34 reward: -71.88\n",
      "Env 7 Episode 34 reward: -66.38\n",
      "Env 0 Episode 35 reward: -36.73\n",
      "Env 1 Episode 35 reward: -42.89\n",
      "Env 2 Episode 35 reward: -50.80\n",
      "Env 3 Episode 35 reward: -43.30\n",
      "Env 4 Episode 35 reward: -44.31\n",
      "Env 5 Episode 35 reward: -43.43\n",
      "Env 6 Episode 35 reward: -45.07\n",
      "Env 7 Episode 35 reward: -48.26\n",
      "Env 0 Episode 36 reward: -36.32\n",
      "Env 1 Episode 36 reward: -35.28\n",
      "Env 2 Episode 36 reward: -43.51\n",
      "Env 3 Episode 36 reward: -60.86\n",
      "Env 4 Episode 36 reward: -51.34\n",
      "Env 5 Episode 36 reward: -41.79\n",
      "Env 6 Episode 36 reward: -42.22\n",
      "Env 7 Episode 36 reward: -30.94\n",
      "Env 0 Episode 37 reward: -51.81\n",
      "Env 1 Episode 37 reward: -39.71\n",
      "Env 2 Episode 37 reward: -41.64\n",
      "Env 3 Episode 37 reward: -52.25\n",
      "Env 4 Episode 37 reward: -40.17\n",
      "Env 5 Episode 37 reward: -45.66\n",
      "Env 6 Episode 37 reward: -61.87\n",
      "Env 7 Episode 37 reward: -50.81\n",
      "Env 0 Episode 38 reward: -58.32\n",
      "Env 1 Episode 38 reward: -54.25\n",
      "Env 2 Episode 38 reward: -30.87\n",
      "Env 3 Episode 38 reward: -65.86\n",
      "Env 4 Episode 38 reward: -53.80\n",
      "Env 5 Episode 38 reward: -55.34\n",
      "Env 6 Episode 38 reward: -46.78\n",
      "Env 7 Episode 38 reward: -42.74\n",
      "Env 0 Episode 39 reward: -56.57\n",
      "Env 1 Episode 39 reward: -65.82\n",
      "Env 2 Episode 39 reward: -69.36\n",
      "Env 3 Episode 39 reward: -51.00\n",
      "Env 4 Episode 39 reward: -65.25\n",
      "Env 5 Episode 39 reward: -35.65\n",
      "Env 6 Episode 39 reward: -38.79\n",
      "Env 7 Episode 39 reward: -65.35\n",
      "Env 0 Episode 40 reward: -46.75\n",
      "Env 1 Episode 40 reward: -42.19\n",
      "Env 2 Episode 40 reward: -34.73\n",
      "Env 3 Episode 40 reward: -49.70\n",
      "Env 4 Episode 40 reward: -63.27\n",
      "Env 5 Episode 40 reward: -61.83\n",
      "Env 6 Episode 40 reward: -62.86\n",
      "Env 7 Episode 40 reward: -73.91\n",
      "Env 0 Episode 41 reward: -59.74\n",
      "Env 1 Episode 41 reward: -47.87\n",
      "Env 2 Episode 41 reward: -67.89\n",
      "Env 3 Episode 41 reward: -67.84\n",
      "Env 4 Episode 41 reward: -34.20\n",
      "Env 5 Episode 41 reward: -52.85\n",
      "Env 6 Episode 41 reward: -69.38\n",
      "Env 7 Episode 41 reward: -69.70\n",
      "Env 0 Episode 42 reward: -45.78\n",
      "Env 1 Episode 42 reward: -62.89\n",
      "Env 2 Episode 42 reward: -23.12\n",
      "Env 3 Episode 42 reward: -46.67\n",
      "Env 4 Episode 42 reward: -51.95\n",
      "Env 5 Episode 42 reward: -52.80\n",
      "Env 6 Episode 42 reward: -32.78\n",
      "Env 7 Episode 42 reward: -53.74\n",
      "Env 0 Episode 43 reward: -42.25\n",
      "Env 1 Episode 43 reward: -43.84\n",
      "Env 2 Episode 43 reward: -44.87\n",
      "Env 3 Episode 43 reward: -69.35\n",
      "Env 4 Episode 43 reward: -47.85\n",
      "Env 5 Episode 43 reward: -52.67\n",
      "Env 6 Episode 43 reward: -53.33\n",
      "Env 7 Episode 43 reward: -42.28\n",
      "Env 0 Episode 44 reward: -47.13\n",
      "Env 1 Episode 44 reward: -57.45\n",
      "Env 2 Episode 44 reward: -41.61\n",
      "Env 3 Episode 44 reward: -53.85\n",
      "Env 4 Episode 44 reward: -26.08\n",
      "Env 5 Episode 44 reward: -52.28\n",
      "Env 6 Episode 44 reward: -35.46\n",
      "Env 7 Episode 44 reward: -51.73\n",
      "Env 0 Episode 45 reward: -66.24\n",
      "Env 1 Episode 45 reward: -48.76\n",
      "Env 2 Episode 45 reward: -69.91\n",
      "Env 3 Episode 45 reward: -48.29\n",
      "Env 4 Episode 45 reward: -50.71\n",
      "Env 5 Episode 45 reward: -58.81\n",
      "Env 6 Episode 45 reward: -50.87\n",
      "Env 7 Episode 45 reward: -69.41\n",
      "Env 0 Episode 46 reward: -51.83\n",
      "Env 1 Episode 46 reward: -26.08\n",
      "Env 2 Episode 46 reward: -48.87\n",
      "Env 3 Episode 46 reward: -45.36\n",
      "Env 4 Episode 46 reward: -37.75\n",
      "Env 5 Episode 46 reward: -55.87\n",
      "Env 6 Episode 46 reward: -32.50\n",
      "Env 7 Episode 46 reward: -57.29\n",
      "Env 0 Episode 47 reward: -51.36\n",
      "Env 1 Episode 47 reward: -59.87\n",
      "Env 2 Episode 47 reward: -50.36\n",
      "Env 3 Episode 47 reward: -44.85\n",
      "Env 4 Episode 47 reward: -62.81\n",
      "Env 5 Episode 47 reward: -41.33\n",
      "Env 6 Episode 47 reward: -24.60\n",
      "Env 7 Episode 47 reward: -55.29\n",
      "Env 0 Episode 48 reward: -51.26\n",
      "Env 1 Episode 48 reward: -70.85\n",
      "Env 2 Episode 48 reward: -48.97\n",
      "Env 3 Episode 48 reward: -37.28\n",
      "Env 4 Episode 48 reward: -56.78\n",
      "Env 5 Episode 48 reward: -38.26\n",
      "Env 6 Episode 48 reward: -51.28\n",
      "Env 7 Episode 48 reward: -55.83\n",
      "Env 0 Episode 49 reward: -62.32\n",
      "Env 1 Episode 49 reward: -68.36\n",
      "Env 2 Episode 49 reward: -53.80\n",
      "Env 3 Episode 49 reward: -62.31\n",
      "Env 4 Episode 49 reward: -54.21\n",
      "Env 5 Episode 49 reward: -30.24\n",
      "Env 6 Episode 49 reward: -46.88\n",
      "Env 7 Episode 49 reward: -36.65\n",
      "Env 0 Episode 50 reward: -15.02\n",
      "Env 1 Episode 50 reward: -61.29\n",
      "Env 2 Episode 50 reward: -32.51\n",
      "Env 3 Episode 50 reward: -50.43\n",
      "Env 4 Episode 50 reward: -47.25\n",
      "Env 5 Episode 50 reward: -49.67\n",
      "Env 6 Episode 50 reward: -54.74\n",
      "Env 7 Episode 50 reward: -50.31\n",
      "Env 0 Episode 51 reward: -64.80\n",
      "Env 1 Episode 51 reward: -57.67\n",
      "Env 2 Episode 51 reward: -70.87\n",
      "Env 3 Episode 51 reward: -47.18\n",
      "Env 4 Episode 51 reward: -49.29\n",
      "Env 5 Episode 51 reward: -25.65\n",
      "Env 6 Episode 51 reward: -58.86\n",
      "Env 7 Episode 51 reward: -71.44\n",
      "Env 0 Episode 52 reward: -63.85\n",
      "Env 1 Episode 52 reward: -40.14\n",
      "Env 2 Episode 52 reward: -65.39\n",
      "Env 3 Episode 52 reward: -48.80\n",
      "Env 4 Episode 52 reward: -27.57\n",
      "Env 5 Episode 52 reward: -23.77\n",
      "Env 6 Episode 52 reward: -72.87\n",
      "Env 7 Episode 52 reward: -33.17\n",
      "Env 0 Episode 53 reward: -56.42\n",
      "Env 1 Episode 53 reward: -26.03\n",
      "Env 2 Episode 53 reward: -57.87\n",
      "Env 3 Episode 53 reward: -43.70\n",
      "Env 4 Episode 53 reward: -53.34\n",
      "Env 5 Episode 53 reward: -38.84\n",
      "Env 6 Episode 53 reward: -29.64\n",
      "Env 7 Episode 53 reward: -63.81\n",
      "Env 0 Episode 54 reward: -34.67\n",
      "Env 1 Episode 54 reward: -61.35\n",
      "Env 2 Episode 54 reward: -49.24\n",
      "Env 3 Episode 54 reward: -55.83\n",
      "Env 4 Episode 54 reward: -68.72\n",
      "Env 5 Episode 54 reward: -51.40\n",
      "Env 6 Episode 54 reward: -56.29\n",
      "Env 7 Episode 54 reward: -53.33\n",
      "Env 0 Episode 55 reward: -44.80\n",
      "Env 1 Episode 55 reward: -25.64\n",
      "Env 2 Episode 55 reward: -32.65\n",
      "Env 3 Episode 55 reward: -45.19\n",
      "Env 4 Episode 55 reward: -30.34\n",
      "Env 5 Episode 55 reward: -36.11\n",
      "Env 6 Episode 55 reward: -64.41\n",
      "Env 7 Episode 55 reward: -53.66\n",
      "Env 0 Episode 56 reward: -61.25\n",
      "Env 1 Episode 56 reward: -46.86\n",
      "Env 2 Episode 56 reward: -71.41\n",
      "Env 3 Episode 56 reward: -47.25\n",
      "Env 4 Episode 56 reward: -52.34\n",
      "Env 5 Episode 56 reward: -34.64\n",
      "Env 6 Episode 56 reward: -49.75\n",
      "Env 7 Episode 56 reward: -68.93\n",
      "Env 0 Episode 57 reward: -66.34\n",
      "Env 1 Episode 57 reward: -49.86\n",
      "Env 2 Episode 57 reward: -53.29\n",
      "Env 3 Episode 57 reward: -82.85\n",
      "Env 4 Episode 57 reward: -48.92\n",
      "Env 5 Episode 57 reward: -28.57\n",
      "Env 6 Episode 57 reward: -58.90\n",
      "Env 7 Episode 57 reward: -51.73\n",
      "Env 0 Episode 58 reward: -38.76\n",
      "Env 1 Episode 58 reward: -50.22\n",
      "Env 2 Episode 58 reward: -68.42\n",
      "Env 3 Episode 58 reward: -72.36\n",
      "Env 4 Episode 58 reward: -29.87\n",
      "Env 5 Episode 58 reward: -35.69\n",
      "Env 6 Episode 58 reward: -41.47\n",
      "Env 7 Episode 58 reward: -25.94\n",
      "Env 0 Episode 59 reward: -38.30\n",
      "Env 1 Episode 59 reward: -31.13\n",
      "Env 2 Episode 59 reward: -60.87\n",
      "Env 3 Episode 59 reward: -41.02\n",
      "Env 4 Episode 59 reward: -49.82\n",
      "Env 5 Episode 59 reward: -95.94\n",
      "Env 6 Episode 59 reward: -68.17\n",
      "Env 7 Episode 59 reward: -61.27\n",
      "Env 0 Episode 60 reward: -33.73\n",
      "Env 1 Episode 60 reward: -50.38\n",
      "Env 2 Episode 60 reward: -41.85\n",
      "Env 3 Episode 60 reward: -43.68\n",
      "Env 4 Episode 60 reward: -60.25\n",
      "Env 5 Episode 60 reward: -52.82\n",
      "Env 6 Episode 60 reward: -49.75\n",
      "Env 7 Episode 60 reward: -48.02\n",
      "Env 0 Episode 61 reward: -61.76\n",
      "Env 1 Episode 61 reward: -51.26\n",
      "Env 2 Episode 61 reward: -45.68\n",
      "Env 3 Episode 61 reward: -24.04\n",
      "Env 4 Episode 61 reward: -74.34\n",
      "Env 5 Episode 61 reward: -44.29\n",
      "Env 6 Episode 61 reward: -39.80\n",
      "Env 7 Episode 61 reward: -68.22\n",
      "Env 0 Episode 62 reward: -41.77\n",
      "Env 1 Episode 62 reward: -48.26\n",
      "Env 2 Episode 62 reward: -52.78\n",
      "Env 3 Episode 62 reward: -33.33\n",
      "Env 4 Episode 62 reward: -53.29\n",
      "Env 5 Episode 62 reward: -52.86\n",
      "Env 6 Episode 62 reward: -52.84\n",
      "Env 7 Episode 62 reward: -49.08\n",
      "Env 0 Episode 63 reward: -32.62\n",
      "Env 1 Episode 63 reward: -40.09\n",
      "Env 2 Episode 63 reward: -38.31\n",
      "Env 3 Episode 63 reward: -55.33\n",
      "Env 4 Episode 63 reward: -35.62\n",
      "Env 5 Episode 63 reward: -65.83\n",
      "Env 6 Episode 63 reward: -46.09\n",
      "Env 7 Episode 63 reward: -46.14\n",
      "Env 0 Episode 64 reward: -47.71\n",
      "Env 1 Episode 64 reward: -47.73\n",
      "Env 2 Episode 64 reward: -34.25\n",
      "Env 3 Episode 64 reward: -27.57\n",
      "Env 4 Episode 64 reward: -67.84\n",
      "Env 5 Episode 64 reward: -49.75\n",
      "Env 6 Episode 64 reward: -17.30\n",
      "Env 7 Episode 64 reward: -77.24\n",
      "Env 0 Episode 65 reward: -52.84\n",
      "Env 1 Episode 65 reward: -60.26\n",
      "Env 2 Episode 65 reward: -50.87\n",
      "Env 3 Episode 65 reward: -38.29\n",
      "Env 4 Episode 65 reward: -53.80\n",
      "Env 5 Episode 65 reward: -44.79\n",
      "Env 6 Episode 65 reward: -20.52\n",
      "Env 7 Episode 65 reward: -57.33\n",
      "Env 0 Episode 66 reward: -38.29\n",
      "Env 1 Episode 66 reward: -48.71\n",
      "Env 2 Episode 66 reward: -51.25\n",
      "Env 3 Episode 66 reward: -47.70\n",
      "Env 4 Episode 66 reward: -53.35\n",
      "Env 5 Episode 66 reward: -49.15\n",
      "Env 6 Episode 66 reward: -58.35\n",
      "Env 7 Episode 66 reward: -43.67\n",
      "Env 0 Episode 67 reward: -49.79\n",
      "Env 1 Episode 67 reward: -70.88\n",
      "Env 2 Episode 67 reward: -51.73\n",
      "Env 3 Episode 67 reward: -50.38\n",
      "Env 4 Episode 67 reward: -48.27\n",
      "Env 5 Episode 67 reward: -26.74\n",
      "Env 6 Episode 67 reward: -33.68\n",
      "Env 7 Episode 67 reward: -50.85\n",
      "Env 0 Episode 68 reward: -47.32\n",
      "Env 1 Episode 68 reward: -61.74\n",
      "Env 2 Episode 68 reward: -53.32\n",
      "Env 3 Episode 68 reward: -36.71\n",
      "Env 4 Episode 68 reward: -54.27\n",
      "Env 5 Episode 68 reward: -48.86\n",
      "Env 6 Episode 68 reward: -38.71\n",
      "Env 7 Episode 68 reward: -85.97\n",
      "Env 0 Episode 69 reward: -33.84\n",
      "Env 1 Episode 69 reward: -36.35\n",
      "Env 2 Episode 69 reward: -28.14\n",
      "Env 3 Episode 69 reward: -41.81\n",
      "Env 4 Episode 69 reward: -56.87\n",
      "Env 5 Episode 69 reward: -32.47\n",
      "Env 6 Episode 69 reward: -22.26\n",
      "Env 7 Episode 69 reward: -49.22\n",
      "Env 0 Episode 70 reward: -44.79\n",
      "Env 1 Episode 70 reward: -39.27\n",
      "Env 2 Episode 70 reward: -53.35\n",
      "Env 3 Episode 70 reward: -57.80\n",
      "Env 4 Episode 70 reward: -47.67\n",
      "Env 5 Episode 70 reward: -57.34\n",
      "Env 6 Episode 70 reward: -56.36\n",
      "Env 7 Episode 70 reward: -49.17\n",
      "Env 0 Episode 71 reward: -24.09\n",
      "Env 1 Episode 71 reward: -51.63\n",
      "Env 2 Episode 71 reward: -51.24\n",
      "Env 3 Episode 71 reward: -58.86\n",
      "Env 4 Episode 71 reward: -44.15\n",
      "Env 5 Episode 71 reward: -41.62\n",
      "Env 6 Episode 71 reward: -66.39\n",
      "Env 7 Episode 71 reward: -53.67\n",
      "Env 0 Episode 72 reward: -43.31\n",
      "Env 1 Episode 72 reward: -42.75\n",
      "Env 2 Episode 72 reward: -47.24\n",
      "Env 3 Episode 72 reward: -49.34\n",
      "Env 4 Episode 72 reward: -43.07\n",
      "Env 5 Episode 72 reward: -71.79\n",
      "Env 6 Episode 72 reward: -59.82\n",
      "Env 7 Episode 72 reward: -61.85\n",
      "Env 0 Episode 73 reward: -36.62\n",
      "Env 1 Episode 73 reward: -30.76\n",
      "Env 2 Episode 73 reward: -39.05\n",
      "Env 3 Episode 73 reward: -36.13\n",
      "Env 4 Episode 73 reward: -64.84\n",
      "Env 5 Episode 73 reward: -24.28\n",
      "Env 6 Episode 73 reward: -41.63\n",
      "Env 7 Episode 73 reward: -29.17\n",
      "Env 0 Episode 74 reward: -48.31\n",
      "Env 1 Episode 74 reward: -64.41\n",
      "Env 2 Episode 74 reward: -70.23\n",
      "Env 3 Episode 74 reward: -63.32\n",
      "Env 4 Episode 74 reward: -51.25\n",
      "Env 5 Episode 74 reward: -60.81\n",
      "Env 6 Episode 74 reward: -44.80\n",
      "Env 7 Episode 74 reward: -71.86\n",
      "Env 0 Episode 75 reward: -26.60\n",
      "Env 1 Episode 75 reward: -75.89\n",
      "Env 2 Episode 75 reward: -41.72\n",
      "Env 3 Episode 75 reward: -9.67\n",
      "Env 4 Episode 75 reward: -37.07\n",
      "Env 5 Episode 75 reward: -48.75\n",
      "Env 6 Episode 75 reward: -36.74\n",
      "Env 7 Episode 75 reward: -47.63\n",
      "Env 0 Episode 76 reward: -46.80\n",
      "Env 1 Episode 76 reward: -49.35\n",
      "Env 2 Episode 76 reward: -50.20\n",
      "Env 3 Episode 76 reward: -55.70\n",
      "Env 4 Episode 76 reward: -52.81\n",
      "Env 5 Episode 76 reward: -24.64\n",
      "Env 6 Episode 76 reward: -38.72\n",
      "Env 7 Episode 76 reward: -39.35\n",
      "Env 0 Episode 77 reward: -41.63\n",
      "Env 1 Episode 77 reward: -42.22\n",
      "Env 2 Episode 77 reward: -60.76\n",
      "Env 3 Episode 77 reward: -40.08\n",
      "Env 4 Episode 77 reward: -49.71\n",
      "Env 5 Episode 77 reward: -53.31\n",
      "Env 6 Episode 77 reward: -54.23\n",
      "Env 7 Episode 77 reward: -61.85\n",
      "Env 0 Episode 78 reward: -44.73\n",
      "Env 1 Episode 78 reward: -44.85\n",
      "Env 2 Episode 78 reward: -47.63\n",
      "Env 3 Episode 78 reward: -40.67\n",
      "Env 4 Episode 78 reward: -40.89\n",
      "Env 5 Episode 78 reward: -38.77\n",
      "Env 6 Episode 78 reward: -57.89\n",
      "Env 7 Episode 78 reward: -64.28\n",
      "Env 0 Episode 79 reward: -24.57\n",
      "Env 1 Episode 79 reward: -40.54\n",
      "Env 2 Episode 79 reward: -61.27\n",
      "Env 3 Episode 79 reward: -56.84\n",
      "Env 4 Episode 79 reward: -38.63\n",
      "Env 5 Episode 79 reward: -40.16\n",
      "Env 6 Episode 79 reward: -42.36\n",
      "Env 7 Episode 79 reward: -50.73\n",
      "Env 0 Episode 80 reward: -54.44\n",
      "Env 1 Episode 80 reward: -51.16\n",
      "Env 2 Episode 80 reward: -31.26\n",
      "Env 3 Episode 80 reward: -59.81\n",
      "Env 4 Episode 80 reward: -57.38\n",
      "Env 5 Episode 80 reward: -41.18\n",
      "Env 6 Episode 80 reward: -56.78\n",
      "Env 7 Episode 80 reward: -54.25\n",
      "Env 0 Episode 81 reward: -50.66\n",
      "Env 1 Episode 81 reward: -33.08\n",
      "Env 2 Episode 81 reward: -28.73\n",
      "Env 3 Episode 81 reward: -26.08\n",
      "Env 4 Episode 81 reward: -31.62\n",
      "Env 5 Episode 81 reward: -32.67\n",
      "Env 6 Episode 81 reward: -56.20\n",
      "Env 7 Episode 81 reward: -39.22\n",
      "Env 0 Episode 82 reward: -19.65\n",
      "Env 1 Episode 82 reward: -70.39\n",
      "Env 2 Episode 82 reward: -39.76\n",
      "Env 3 Episode 82 reward: -28.41\n",
      "Env 4 Episode 82 reward: -47.18\n",
      "Env 5 Episode 82 reward: -69.85\n",
      "Env 6 Episode 82 reward: -36.46\n",
      "Env 7 Episode 82 reward: -23.88\n",
      "Env 0 Episode 83 reward: -42.13\n",
      "Env 1 Episode 83 reward: -67.35\n",
      "Env 2 Episode 83 reward: -47.71\n",
      "Env 3 Episode 83 reward: -58.26\n",
      "Env 4 Episode 83 reward: -41.28\n",
      "Env 5 Episode 83 reward: -44.76\n",
      "Env 6 Episode 83 reward: -28.69\n",
      "Env 7 Episode 83 reward: -57.85\n",
      "Env 0 Episode 84 reward: -33.25\n",
      "Env 1 Episode 84 reward: -69.74\n",
      "Env 2 Episode 84 reward: -18.94\n",
      "Env 3 Episode 84 reward: -46.11\n",
      "Env 4 Episode 84 reward: -60.38\n",
      "Env 5 Episode 84 reward: -77.39\n",
      "Env 6 Episode 84 reward: -28.23\n",
      "Env 7 Episode 84 reward: -40.83\n",
      "Env 0 Episode 85 reward: -36.15\n",
      "Env 1 Episode 85 reward: -62.85\n",
      "Env 2 Episode 85 reward: -43.03\n",
      "Env 3 Episode 85 reward: -0.47\n",
      "Env 4 Episode 85 reward: -23.74\n",
      "Env 5 Episode 85 reward: -46.19\n",
      "Env 6 Episode 85 reward: -38.24\n",
      "Env 7 Episode 85 reward: -33.26\n",
      "Env 0 Episode 86 reward: -40.66\n",
      "Env 1 Episode 86 reward: -37.11\n",
      "Env 2 Episode 86 reward: -51.81\n",
      "Env 3 Episode 86 reward: -38.24\n",
      "Env 4 Episode 86 reward: -49.90\n",
      "Env 5 Episode 86 reward: -18.06\n",
      "Env 6 Episode 86 reward: -27.62\n",
      "Env 7 Episode 86 reward: -32.09\n",
      "Env 0 Episode 87 reward: -74.83\n",
      "Env 1 Episode 87 reward: -65.38\n",
      "Env 2 Episode 87 reward: -29.50\n",
      "Env 3 Episode 87 reward: -59.91\n",
      "Env 4 Episode 87 reward: -30.26\n",
      "Env 5 Episode 87 reward: -64.36\n",
      "Env 6 Episode 87 reward: -51.16\n",
      "Env 7 Episode 87 reward: -51.21\n",
      "Env 0 Episode 88 reward: -42.34\n",
      "Env 1 Episode 88 reward: -48.68\n",
      "Env 2 Episode 88 reward: -73.94\n",
      "Env 3 Episode 88 reward: -29.04\n",
      "Env 4 Episode 88 reward: -46.62\n",
      "Env 5 Episode 88 reward: -57.77\n",
      "Env 6 Episode 88 reward: -26.19\n",
      "Env 7 Episode 88 reward: -43.35\n",
      "Env 0 Episode 89 reward: -33.58\n",
      "Env 1 Episode 89 reward: -15.73\n",
      "Env 2 Episode 89 reward: -38.83\n",
      "Env 3 Episode 89 reward: -53.35\n",
      "Env 4 Episode 89 reward: -48.83\n",
      "Env 5 Episode 89 reward: -53.83\n",
      "Env 6 Episode 89 reward: -43.24\n",
      "Env 7 Episode 89 reward: -52.65\n",
      "Env 0 Episode 90 reward: -35.12\n",
      "Env 1 Episode 90 reward: -72.40\n",
      "Env 2 Episode 90 reward: -47.78\n",
      "Env 3 Episode 90 reward: -54.38\n",
      "Env 4 Episode 90 reward: -54.75\n",
      "Env 5 Episode 90 reward: -31.66\n",
      "Env 6 Episode 90 reward: -34.25\n",
      "Env 7 Episode 90 reward: -66.71\n",
      "Env 0 Episode 91 reward: -69.31\n",
      "Env 1 Episode 91 reward: -52.69\n",
      "Env 2 Episode 91 reward: -54.70\n",
      "Env 3 Episode 91 reward: -51.17\n",
      "Env 4 Episode 91 reward: -28.28\n",
      "Env 5 Episode 91 reward: -56.84\n",
      "Env 6 Episode 91 reward: -41.79\n",
      "Env 7 Episode 91 reward: -66.88\n",
      "Env 0 Episode 92 reward: -45.68\n",
      "Env 1 Episode 92 reward: -85.44\n",
      "Env 2 Episode 92 reward: -51.83\n",
      "Env 3 Episode 92 reward: -29.08\n",
      "Env 4 Episode 92 reward: -49.18\n",
      "Env 5 Episode 92 reward: -41.22\n",
      "Env 6 Episode 92 reward: -42.34\n",
      "Env 7 Episode 92 reward: -42.20\n",
      "Env 0 Episode 93 reward: -44.74\n",
      "Env 1 Episode 93 reward: -29.38\n",
      "Env 2 Episode 93 reward: -65.31\n",
      "Env 3 Episode 93 reward: -73.89\n",
      "Env 4 Episode 93 reward: -74.40\n",
      "Env 5 Episode 93 reward: -55.64\n",
      "Env 6 Episode 93 reward: -62.25\n",
      "Env 7 Episode 93 reward: -38.49\n",
      "Env 0 Episode 94 reward: -49.28\n",
      "Env 1 Episode 94 reward: -59.29\n",
      "Env 2 Episode 94 reward: -12.82\n",
      "Env 3 Episode 94 reward: -56.80\n",
      "Env 4 Episode 94 reward: -50.28\n",
      "Env 5 Episode 94 reward: -37.57\n",
      "Env 6 Episode 94 reward: -53.77\n",
      "Env 7 Episode 94 reward: -51.34\n",
      "Env 0 Episode 95 reward: -39.30\n",
      "Env 1 Episode 95 reward: -48.71\n",
      "Env 2 Episode 95 reward: -42.24\n",
      "Env 3 Episode 95 reward: -41.63\n",
      "Env 4 Episode 95 reward: -64.44\n",
      "Env 5 Episode 95 reward: -57.30\n",
      "Env 6 Episode 95 reward: -33.63\n",
      "Env 7 Episode 95 reward: -77.88\n",
      "Env 0 Episode 96 reward: -36.83\n",
      "Env 1 Episode 96 reward: -55.33\n",
      "Env 2 Episode 96 reward: -53.31\n",
      "Env 3 Episode 96 reward: -48.56\n",
      "Env 4 Episode 96 reward: -24.26\n",
      "Env 5 Episode 96 reward: -61.41\n",
      "Env 6 Episode 96 reward: -46.25\n",
      "Env 7 Episode 96 reward: -35.75\n",
      "Env 0 Episode 97 reward: -49.80\n",
      "Env 1 Episode 97 reward: -65.85\n",
      "Env 2 Episode 97 reward: -41.73\n",
      "Env 3 Episode 97 reward: -38.53\n",
      "Env 4 Episode 97 reward: -66.43\n",
      "Env 5 Episode 97 reward: -63.38\n",
      "Env 6 Episode 97 reward: -55.89\n",
      "Env 7 Episode 97 reward: -39.82\n",
      "Env 0 Episode 98 reward: -41.31\n",
      "Env 1 Episode 98 reward: -52.83\n",
      "Env 2 Episode 98 reward: -47.89\n",
      "Env 3 Episode 98 reward: -62.37\n",
      "Env 4 Episode 98 reward: -56.80\n",
      "Env 5 Episode 98 reward: -34.61\n",
      "Env 6 Episode 98 reward: -53.43\n",
      "Env 7 Episode 98 reward: -43.70\n",
      "Env 0 Episode 99 reward: -50.75\n",
      "Env 1 Episode 99 reward: -37.69\n",
      "Env 2 Episode 99 reward: -66.40\n",
      "Env 3 Episode 99 reward: -60.87\n",
      "Env 4 Episode 99 reward: -58.35\n",
      "Env 5 Episode 99 reward: -55.85\n",
      "Env 6 Episode 99 reward: -38.72\n",
      "Env 7 Episode 99 reward: -29.60\n",
      "Env 0 Episode 100 reward: -62.36\n",
      "Env 1 Episode 100 reward: -59.86\n",
      "Env 2 Episode 100 reward: -53.86\n",
      "Env 3 Episode 100 reward: -31.00\n",
      "Env 4 Episode 100 reward: -59.27\n",
      "Env 5 Episode 100 reward: -41.77\n",
      "Env 6 Episode 100 reward: -35.80\n",
      "Env 7 Episode 100 reward: -43.81\n",
      "Env 0 Episode 101 reward: -32.83\n",
      "Env 1 Episode 101 reward: -68.34\n",
      "Env 2 Episode 101 reward: -47.82\n",
      "Env 3 Episode 101 reward: -30.92\n",
      "Env 4 Episode 101 reward: -31.18\n",
      "Env 5 Episode 101 reward: -55.30\n",
      "Env 6 Episode 101 reward: -41.34\n",
      "Env 7 Episode 101 reward: -77.38\n",
      "Env 0 Episode 102 reward: -45.47\n",
      "Env 1 Episode 102 reward: -43.83\n",
      "Env 2 Episode 102 reward: -33.22\n",
      "Env 3 Episode 102 reward: -62.31\n",
      "Env 4 Episode 102 reward: -42.71\n",
      "Env 5 Episode 102 reward: -44.32\n",
      "Env 6 Episode 102 reward: -49.27\n",
      "Env 7 Episode 102 reward: -32.57\n",
      "Env 0 Episode 103 reward: -51.70\n",
      "Env 1 Episode 103 reward: -47.24\n",
      "Env 2 Episode 103 reward: -55.21\n",
      "Env 3 Episode 103 reward: -28.70\n",
      "Env 4 Episode 103 reward: -47.31\n",
      "Env 5 Episode 103 reward: -27.27\n",
      "Env 6 Episode 103 reward: -25.62\n",
      "Env 7 Episode 103 reward: -56.81\n",
      "Env 0 Episode 104 reward: -38.16\n",
      "Env 1 Episode 104 reward: -35.27\n",
      "Env 2 Episode 104 reward: -38.17\n",
      "Env 3 Episode 104 reward: -33.71\n",
      "Env 4 Episode 104 reward: -45.09\n",
      "Env 5 Episode 104 reward: -41.13\n",
      "Env 6 Episode 104 reward: -44.78\n",
      "Env 7 Episode 104 reward: -28.71\n",
      "Env 0 Episode 105 reward: -64.30\n",
      "Env 1 Episode 105 reward: -43.66\n",
      "Env 2 Episode 105 reward: -63.84\n",
      "Env 3 Episode 105 reward: -46.75\n",
      "Env 4 Episode 105 reward: -41.76\n",
      "Env 5 Episode 105 reward: -63.80\n",
      "Env 6 Episode 105 reward: -49.74\n",
      "Env 7 Episode 105 reward: -39.51\n",
      "Env 0 Episode 106 reward: -37.26\n",
      "Env 1 Episode 106 reward: -47.18\n",
      "Env 2 Episode 106 reward: -58.72\n",
      "Env 3 Episode 106 reward: -40.44\n",
      "Env 4 Episode 106 reward: -49.72\n",
      "Env 5 Episode 106 reward: -44.77\n",
      "Env 6 Episode 106 reward: -78.37\n",
      "Env 7 Episode 106 reward: -41.72\n",
      "Env 0 Episode 107 reward: -31.78\n",
      "Env 1 Episode 107 reward: -26.74\n",
      "Env 2 Episode 107 reward: -52.84\n",
      "Env 3 Episode 107 reward: -48.82\n",
      "Env 4 Episode 107 reward: -49.28\n",
      "Env 5 Episode 107 reward: -38.17\n",
      "Env 6 Episode 107 reward: -41.72\n",
      "Env 7 Episode 107 reward: -61.84\n",
      "Env 0 Episode 108 reward: -47.86\n",
      "Env 1 Episode 108 reward: -48.37\n",
      "Env 2 Episode 108 reward: -36.34\n",
      "Env 3 Episode 108 reward: -42.29\n",
      "Env 4 Episode 108 reward: -46.15\n",
      "Env 5 Episode 108 reward: -55.87\n",
      "Env 6 Episode 108 reward: -66.84\n",
      "Env 7 Episode 108 reward: -49.31\n"
     ]
    }
   ],
   "source": [
    "####### Improved BTC DQN with Multi-Env Randomized Start\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# =========================\n",
    "# Constants\n",
    "# =========================\n",
    "STABLE_THRESHOLD = 0.001  # 0.1% movement considered stable\n",
    "\n",
    "# =========================\n",
    "# Custom Callback for Multi-Env Logging\n",
    "# =========================\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.current_rewards = {}\n",
    "        self.episode_counts = {}\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        infos = self.locals['infos']\n",
    "        rewards = self.locals['rewards']\n",
    "        dones = self.locals['dones']\n",
    "\n",
    "        for idx, info in enumerate(infos):\n",
    "            ep_id = idx\n",
    "            # Initialize tracking\n",
    "            if ep_id not in self.current_rewards:\n",
    "                self.current_rewards[ep_id] = 0\n",
    "                self.episode_counts[ep_id] = 0\n",
    "\n",
    "            self.current_rewards[ep_id] += rewards[idx]\n",
    "\n",
    "            if dones[idx]:\n",
    "                print(f\"Env {ep_id} Episode {self.episode_counts[ep_id]+1} reward: {self.current_rewards[ep_id]:.2f}\")\n",
    "                self.episode_rewards.append(self.current_rewards[ep_id])\n",
    "                self.current_rewards[ep_id] = 0\n",
    "                self.episode_counts[ep_id] += 1\n",
    "\n",
    "        return True\n",
    "\n",
    "# =========================\n",
    "# Custom BTC Trading Environment with Random Start\n",
    "# =========================\n",
    "class BTCTradingEnv(gym.Env):\n",
    "    def __init__(self, data, lookback_window=12, transaction_cost=0.001):\n",
    "        super(BTCTradingEnv, self).__init__()\n",
    "        self.data = data.drop(columns=['OpenTime']).astype(np.float32)\n",
    "        self.lookback = lookback_window\n",
    "        self.transaction_cost = transaction_cost\n",
    "\n",
    "        obs_shape = (self.lookback * self.data.shape[1],)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=obs_shape, dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(3)  # Hold, Buy, Sell\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.steps_taken = 0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # Random start\n",
    "        self.current_step = np.random.randint(self.lookback, len(self.data) - 200)\n",
    "        self.steps_taken = 0\n",
    "        obs = self._get_obs()\n",
    "        return obs, {}\n",
    "\n",
    "    def _calculate_reward(self, action, delta_pct):\n",
    "        up_threshold = 0.001\n",
    "        down_threshold = -0.001\n",
    "        reward = 0.0\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            reward = 1 + abs(delta_pct) if delta_pct > up_threshold else -1\n",
    "        elif action == 2:  # Sell\n",
    "            reward = 1 + abs(delta_pct) if delta_pct < down_threshold else -1\n",
    "        else:  # Hold\n",
    "            reward = 0.5 if abs(delta_pct) <= up_threshold else -0.5\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        current_close = self.data.iloc[self.current_step]['Close']\n",
    "        next_close = self.data.iloc[self.current_step + 1]['Close']\n",
    "        delta_pct = (next_close - current_close) / current_close\n",
    "\n",
    "        reward = self._calculate_reward(action, delta_pct)\n",
    "\n",
    "        info = {\n",
    "            'step': self.current_step,\n",
    "            'delta_pct': delta_pct,\n",
    "            'reward': reward,\n",
    "            'action': action\n",
    "        }\n",
    "\n",
    "        self.current_step += 1\n",
    "        self.steps_taken += 1\n",
    "\n",
    "        # Episode length capped at 200 steps or end of data\n",
    "        terminated = self.steps_taken >= 200 or self.current_step >= len(self.data) - 1\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._get_obs() if not terminated else np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        start = self.current_step - self.lookback\n",
    "        end = self.current_step\n",
    "        window = self.data.iloc[start:end].values\n",
    "        return window.flatten().astype(np.float32)\n",
    "\n",
    "# =========================\n",
    "# Evaluation Function\n",
    "# =========================\n",
    "def evaluate_model(model, vec_env):\n",
    "    obs = vec_env.reset()\n",
    "    terminated = False\n",
    "    total_steps = 0\n",
    "    num_correct = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    print(\"Starting evaluation episode\")\n",
    "    while not terminated:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, term, trunc, infos = vec_env.step(action)\n",
    "        terminated = term[0] or trunc[0]\n",
    "\n",
    "        info = infos[0]\n",
    "        reward = rewards[0]\n",
    "        delta_pct = info['delta_pct'] * 100\n",
    "        step = info['step']\n",
    "        action_val = action[0]\n",
    "\n",
    "        action_str = ['Hold', 'Buy', 'Sell'][action_val]\n",
    "        movement = 'stable' if abs(delta_pct) < STABLE_THRESHOLD*100 else ('up' if delta_pct>0 else 'down')\n",
    "        print(f\"Step {step}, chosen action {action_str}, next price movement {movement} ({delta_pct:.2f}%), reward {reward:.4f}\")\n",
    "\n",
    "        total_steps += 1\n",
    "        total_reward += reward\n",
    "        if reward >= 0:\n",
    "            num_correct += 1\n",
    "\n",
    "    accuracy = (num_correct / total_steps) * 100 if total_steps > 0 else 0\n",
    "    print(\"Evaluation completed.\")\n",
    "    print(f\"Final evaluation accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Total cumulative reward: {total_reward:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# Main Training & Evaluation\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('BTC-USD_with_Indicators.csv')\n",
    "\n",
    "    # 8 parallel environments\n",
    "    vec_env = DummyVecEnv([lambda: BTCTradingEnv(df) for _ in range(8)])\n",
    "\n",
    "    model_path = 'btc_dqn_agent.zip'\n",
    "    trained = False\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        model = DQN.load(model_path, env=vec_env)\n",
    "        print(\"Loaded existing model.\")\n",
    "    else:\n",
    "        model = DQN(\n",
    "            \"MlpPolicy\",\n",
    "            vec_env,\n",
    "            verbose=0,\n",
    "            learning_rate=1e-4,\n",
    "            buffer_size=10000,\n",
    "            batch_size=32,\n",
    "            train_freq=4,\n",
    "            target_update_interval=500,\n",
    "            gamma=0.99\n",
    "        )\n",
    "\n",
    "        callback = CustomCallback()\n",
    "        total_timesteps = 20 * (len(df) - 1)\n",
    "\n",
    "        model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "        model.save(model_path)\n",
    "        print(\"Trained and saved new model.\")\n",
    "\n",
    "        # Plot rewards\n",
    "        plt.plot(callback.episode_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Training Reward History')\n",
    "        plt.show()\n",
    "\n",
    "    # Evaluate trained model\n",
    "    evaluate_model(model, vec_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642dbf19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
