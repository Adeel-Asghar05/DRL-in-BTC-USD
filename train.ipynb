{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ebe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Custom callback for printing during training and collecting reward history\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.episode = 0\n",
    "        self.num_correct = 0\n",
    "        self.total_steps = 0\n",
    "        self.current_ep_reward = 0\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Access locals from the training process\n",
    "        info = self.locals['infos'][0]\n",
    "        action = self.locals['actions'][0]\n",
    "        reward = self.locals['rewards'][0]\n",
    "        done = self.locals['dones'][0]\n",
    "        \n",
    "        # Extract info from environment step\n",
    "        step = info['step']\n",
    "        delta = info['delta']\n",
    "        \n",
    "        # Map action to string\n",
    "        action_str = ['Hold', 'Buy', 'Sell'][action]\n",
    "        \n",
    "        # Determine price movement\n",
    "        if abs(delta) < info['threshold']:\n",
    "            movement = 'stable'\n",
    "        elif delta > 0:\n",
    "            movement = 'up'\n",
    "        else:\n",
    "            movement = 'down'\n",
    "        movement_str = f\"{movement} ({delta:.2f})\"\n",
    "        \n",
    "        # Print per step information\n",
    "        print(f\"Step {step}, chosen action {action_str}, next price movement {movement_str}, reward {reward}\")\n",
    "        \n",
    "        # Update accuracy counters\n",
    "        self.total_steps += 1\n",
    "        if reward == 1:\n",
    "            self.num_correct += 1\n",
    "        accuracy = (self.num_correct / self.total_steps) * 100\n",
    "        print(f\"Running accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Update current episode reward\n",
    "        self.current_ep_reward += reward\n",
    "        \n",
    "        # Check if step is the start of an episode\n",
    "        if step == 0:\n",
    "            print(f\"Episode {self.episode + 1}\")\n",
    "        \n",
    "        # Handle episode end\n",
    "        if done:\n",
    "            print(f\"Episode {self.episode + 1} completed.\")\n",
    "            self.episode_rewards.append(self.current_ep_reward)\n",
    "            self.current_ep_reward = 0\n",
    "            self.episode += 1\n",
    "        \n",
    "        return True\n",
    "\n",
    "# Custom Gymnasium environment for BTC/USD trading\n",
    "class BTCTradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(BTCTradingEnv, self).__init__()\n",
    "        # Store the data (dropping OpenTime)\n",
    "        self.data = data.drop(columns=['OpenTime'])\n",
    "        # Ensure all columns are float\n",
    "        self.data = self.data.astype(np.float32)\n",
    "        # Observation space: 9 features\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(9,), dtype=np.float32)\n",
    "        # Action space: 0=Hold, 1=Buy, 2=Sell\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        # Current step in the episode\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        # Reset to the beginning of the dataset\n",
    "        self.current_step = 0\n",
    "        obs = self._get_obs()\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Get current and next close prices\n",
    "        current_close = self.data.iloc[self.current_step]['Close']\n",
    "        next_close = self.data.iloc[self.current_step + 1]['Close']\n",
    "        delta = next_close - current_close\n",
    "        \n",
    "        # Define relative threshold for 'small' change (0.1% of current price)\n",
    "        threshold = 0.001 * current_close\n",
    "        \n",
    "        # Compute reward based on action and price movement\n",
    "        if action == 0:  # Hold\n",
    "            reward = 1 if abs(delta) < threshold else -1\n",
    "        elif action == 1:  # Buy\n",
    "            reward = 1 if delta > 0 else -1\n",
    "        elif action == 2:  # Sell\n",
    "            reward = 1 if delta < 0 else -1\n",
    "        \n",
    "        # Prepare info dict for callback/evaluation\n",
    "        info = {\n",
    "            'step': self.current_step,\n",
    "            'delta': delta,\n",
    "            'threshold': threshold,\n",
    "            'reward': reward,\n",
    "            'action': action\n",
    "        }\n",
    "        \n",
    "        # Advance to the next step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Check if episode is done (reached end of dataset)\n",
    "        terminated = self.current_step >= len(self.data) - 1\n",
    "        truncated = False\n",
    "        \n",
    "        # Get next observation if not done\n",
    "        obs = self._get_obs() if not terminated else np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Return current row as observation\n",
    "        return self.data.iloc[self.current_step].values\n",
    "\n",
    "# Function to run evaluation with prints\n",
    "def evaluate_model(model, vec_env):\n",
    "    obs = vec_env.reset()\n",
    "    terminated = False\n",
    "    total_steps = 0\n",
    "    num_correct = 0\n",
    "    print(\"Starting evaluation episode\")\n",
    "    while not terminated:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, term, trunc, infos = vec_env.step(action)\n",
    "        terminated = term[0] or trunc[0]  # Since single env\n",
    "        info = infos[0]\n",
    "        reward = rewards[0]\n",
    "        delta = info['delta']\n",
    "        step = info['step']\n",
    "        action_val = action[0]  # action is array\n",
    "        \n",
    "        # Map action to string\n",
    "        action_str = ['Hold', 'Buy', 'Sell'][action_val]\n",
    "        \n",
    "        # Determine price movement\n",
    "        if abs(delta) < info['threshold']:\n",
    "            movement = 'stable'\n",
    "        elif delta > 0:\n",
    "            movement = 'up'\n",
    "        else:\n",
    "            movement = 'down'\n",
    "        movement_str = f\"{movement} ({delta:.2f})\"\n",
    "        \n",
    "        # Print per step\n",
    "        print(f\"Step {step}, chosen action {action_str}, next price movement {movement_str}, reward {reward}\")\n",
    "        \n",
    "        # Update accuracy\n",
    "        total_steps += 1\n",
    "        if reward == 1:\n",
    "            num_correct += 1\n",
    "        accuracy = (num_correct / total_steps) * 100 if total_steps > 0 else 0\n",
    "        print(f\"Running accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    print(\"Evaluation completed.\")\n",
    "    print(f\"Final evaluation accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv('BTC-USD_with_Indicators.csv')\n",
    "    \n",
    "    # Create the environment\n",
    "    env = BTCTradingEnv(df)\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    model_path = 'btc_dqn_agent.zip'\n",
    "    trained = False\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        # Load the model if it exists\n",
    "        model = DQN.load(model_path, env=vec_env)\n",
    "        print(\"Loaded existing model.\")\n",
    "    else:\n",
    "        # Create and train the model\n",
    "        model = DQN(\"MlpPolicy\", vec_env, verbose=1)\n",
    "        callback = CustomCallback()\n",
    "        \n",
    "        # Compute total timesteps (e.g., 5 full episodes)\n",
    "        steps_per_episode = len(df) - 1\n",
    "        total_timesteps = 5 * steps_per_episode\n",
    "        \n",
    "        model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "        model.save(model_path)\n",
    "        print(\"Trained and saved new model.\")\n",
    "        trained = True\n",
    "        \n",
    "        # Plot training reward history\n",
    "        plt.plot(callback.episode_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Training Reward History')\n",
    "        plt.show()\n",
    "    \n",
    "    # Run evaluation (with prints)\n",
    "    evaluate_model(model, vec_env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
