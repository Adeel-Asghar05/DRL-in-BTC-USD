{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a623923c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "####### Improved reward system with proper structure\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# =========================\n",
    "# Constants\n",
    "# =========================\n",
    "STABLE_THRESHOLD = 0.001  # 0.1% movement considered stable\n",
    "# =========================\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.episode = 0\n",
    "        self.num_correct = 0\n",
    "        self.total_steps = 0\n",
    "        self.current_ep_reward = 0\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        info = self.locals['infos'][0]\n",
    "        action = self.locals['actions'][0]\n",
    "        reward = self.locals['rewards'][0]\n",
    "        done = self.locals['dones'][0]\n",
    "\n",
    "        step = info['step']\n",
    "        delta_pct = info['delta_pct'] * 100  # convert to % for readability\n",
    "        action_str = ['Hold', 'Buy', 'Sell'][action]\n",
    "\n",
    "        if abs(delta_pct) < info['threshold_pct'] * 100:\n",
    "            movement = 'stable'\n",
    "        elif delta_pct > 0:\n",
    "            movement = 'up'\n",
    "        else:\n",
    "            movement = 'down'\n",
    "        movement_str = f\"{movement} ({delta_pct:.2f}%)\"\n",
    "\n",
    "        print(f\"Step {step}, chosen action {action_str}, next price movement {movement_str}, reward {reward:.4f}\")\n",
    "\n",
    "        # Track accuracy (reward >= 0 considered correct)\n",
    "        # self.total_steps += 1\n",
    "        # if reward >= 0:\n",
    "        #     self.num_correct += 1\n",
    "        # accuracy = (self.num_correct / self.total_steps) * 100\n",
    "        # print(f\"Running accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        # self.current_ep_reward += reward\n",
    "\n",
    "        if step == 0:\n",
    "            print(f\"Episode {self.episode + 1}\")\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode {self.episode + 1} completed.\")\n",
    "            self.episode_rewards.append(self.current_ep_reward)\n",
    "            self.current_ep_reward = 0\n",
    "            self.episode += 1\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Custom BTC Trading Environment\n",
    "# =========================\n",
    "class BTCTradingEnv(gym.Env):\n",
    "    def __init__(self, data, lookback_window=12, transaction_cost=0.001):\n",
    "        super(BTCTradingEnv, self).__init__()\n",
    "        self.data = data.drop(columns=['OpenTime'])\n",
    "        self.data = self.data.astype(np.float32)\n",
    "\n",
    "        self.lookback = lookback_window\n",
    "        self.transaction_cost = transaction_cost\n",
    "\n",
    "        obs_shape = (self.lookback * self.data.shape[1],)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=obs_shape, dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(3)  # Hold, Buy, Sell\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.current_step = self.lookback  # start after enough history\n",
    "        obs = self._get_obs()\n",
    "        return obs, {}\n",
    "\n",
    "    def _calculate_reward(self, action, delta_pct):\n",
    "    # Define thresholds\n",
    "        up_threshold = 0.001   # >0.1% = up\n",
    "        down_threshold = -0.001 # <-0.1% = down\n",
    "\n",
    "        reward = 0.0\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            if delta_pct > up_threshold:\n",
    "                reward = 1 + abs(delta_pct)  # correct, stronger move = bigger reward\n",
    "            else:\n",
    "                reward = -1                  # wrong prediction\n",
    "\n",
    "        elif action == 2:  # Sell\n",
    "            if delta_pct < down_threshold:\n",
    "                reward = 1 + abs(delta_pct)  # correct, stronger down = bigger reward\n",
    "            else:\n",
    "                reward = -1\n",
    "\n",
    "        else:  # Hold\n",
    "            if abs(delta_pct) <= up_threshold:   # stable market\n",
    "                reward = 0.5                     # small positive reward\n",
    "            else:\n",
    "                reward = -0.5                    # should have predicted moves\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        current_close = self.data.iloc[self.current_step]['Close']\n",
    "        next_close = self.data.iloc[self.current_step + 1]['Close']\n",
    "        delta = next_close - current_close\n",
    "        delta_pct = delta / current_close\n",
    "\n",
    "        # Calculate reward using improved system\n",
    "        reward = self._calculate_reward(action, delta_pct)\n",
    "\n",
    "        info = {\n",
    "            'step': self.current_step,\n",
    "            'delta': delta,\n",
    "            'delta_pct': delta_pct,\n",
    "            'threshold_pct': STABLE_THRESHOLD,\n",
    "            'reward': reward,\n",
    "            'action': action,\n",
    "        }\n",
    "\n",
    "        self.current_step += 1\n",
    "        terminated = self.current_step >= len(self.data) - 1\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._get_obs() if not terminated else np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        start = self.current_step - self.lookback\n",
    "        end = self.current_step\n",
    "        window = self.data.iloc[start:end].values\n",
    "        return window.flatten().astype(np.float32)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Evaluation function\n",
    "# =========================\n",
    "def evaluate_model(model, vec_env):\n",
    "    obs = vec_env.reset()\n",
    "    terminated = False\n",
    "    total_steps = 0\n",
    "    num_correct = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    print(\"Starting evaluation episode\")\n",
    "    while not terminated:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, term, trunc, infos = vec_env.step(action)\n",
    "        terminated = term[0] or trunc[0]\n",
    "\n",
    "        info = infos[0]\n",
    "        reward = rewards[0]\n",
    "        delta_pct = info['delta_pct'] * 100\n",
    "        step = info['step']\n",
    "        action_val = action[0]\n",
    "\n",
    "        action_str = ['Hold', 'Buy', 'Sell'][action_val]\n",
    "        if abs(delta_pct) < info['threshold_pct'] * 100:\n",
    "            movement = 'stable'\n",
    "        elif delta_pct > 0:\n",
    "            movement = 'up'\n",
    "        else:\n",
    "            movement = 'down'\n",
    "\n",
    "        print(f\"Step {step}, chosen action {action_str}, next price movement {movement} ({delta_pct:.2f}%), reward {reward:.4f}\")\n",
    "\n",
    "        total_steps += 1\n",
    "        total_reward += reward\n",
    "        if reward >= 0:\n",
    "            num_correct += 1\n",
    "\n",
    "    accuracy = (num_correct / total_steps) * 100 if total_steps > 0 else 0\n",
    "    print(\"Evaluation completed.\")\n",
    "    print(f\"Final evaluation accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Total cumulative reward: {total_reward:.4f}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('BTC-USD_with_Indicators.csv')\n",
    "    env = BTCTradingEnv(df, lookback_window=12, transaction_cost=0.001)\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    model_path = 'btc_dqn_agent.zip'\n",
    "    trained = False\n",
    "    if os.path.exists(model_path):\n",
    "        model = DQN.load(model_path, env=vec_env)\n",
    "        print(\"Loaded existing model.\")\n",
    "    else:\n",
    "        # model = DQN(\"MlpPolicy\", vec_env, verbose=1)\n",
    "        model = DQN(\n",
    "            \"MlpPolicy\",\n",
    "            vec_env,\n",
    "            verbose=1,\n",
    "            learning_rate=1e-4,\n",
    "            buffer_size=50000,\n",
    "            learning_starts=1000,\n",
    "            batch_size=32,\n",
    "            tau=0.05,\n",
    "            gamma=0.99,\n",
    "            train_freq=4,\n",
    "            target_update_interval=500,\n",
    "            exploration_fraction=0.2,\n",
    "            exploration_final_eps=0.05,\n",
    "        )\n",
    "        callback = CustomCallback()\n",
    "\n",
    "        steps_per_episode = len(df) - 1\n",
    "        total_timesteps = 20 * steps_per_episode\n",
    "\n",
    "        model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "        model.save(model_path)\n",
    "        print(\"Trained and saved new model.\")\n",
    "        trained = True\n",
    "\n",
    "        plt.plot(callback.episode_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Training Reward History')\n",
    "        plt.show()\n",
    "\n",
    "    evaluate_model(model, vec_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3fc981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
