{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c148544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import optuna\n",
    "\n",
    "# =========================\n",
    "# Custom callback for printing during training\n",
    "# =========================\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.episode = 0\n",
    "        self.num_correct = 0\n",
    "        self.total_steps = 0\n",
    "        self.current_ep_reward = 0\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        info = self.locals['infos'][0]\n",
    "        action = self.locals['actions'][0]\n",
    "        reward = self.locals['rewards'][0]\n",
    "        done = self.locals['dones'][0]\n",
    "\n",
    "        step = info['step']\n",
    "        delta_pct = info['delta_pct'] * 100  # convert to % for readability\n",
    "        action_str = ['Hold', 'Buy', 'Sell'][action]\n",
    "\n",
    "        if abs(delta_pct) < info['threshold_pct'] * 100:\n",
    "            movement = 'stable'\n",
    "        elif delta_pct > 0:\n",
    "            movement = 'up'\n",
    "        else:\n",
    "            movement = 'down'\n",
    "        movement_str = f\"{movement} ({delta_pct:.2f}%)\"\n",
    "\n",
    "        print(f\"Step {step}, chosen action {action_str}, next price movement {movement_str}, reward {reward:.4f}\")\n",
    "\n",
    "        # Track accuracy (reward >= 0 considered correct)\n",
    "        self.total_steps += 1\n",
    "        if reward >= 0:\n",
    "            self.num_correct += 1\n",
    "        accuracy = (self.num_correct / self.total_steps) * 100\n",
    "        print(f\"Running accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        self.current_ep_reward += reward\n",
    "\n",
    "        if step == 0:\n",
    "            print(f\"Episode {self.episode + 1}\")\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode {self.episode + 1} completed.\")\n",
    "            self.episode_rewards.append(self.current_ep_reward)\n",
    "            self.current_ep_reward = 0\n",
    "            self.episode += 1\n",
    "\n",
    "        return True\n",
    "\n",
    "# =========================\n",
    "# Custom BTC Trading Environment\n",
    "# =========================\n",
    "class BTCTradingEnv(gym.Env):\n",
    "    def __init__(self, data, lookback_window=12, transaction_cost=0.001):\n",
    "        super(BTCTradingEnv, self).__init__()\n",
    "        self.data = data.drop(columns=['OpenTime'])\n",
    "        self.data = self.data.astype(np.float32)\n",
    "\n",
    "        self.lookback = lookback_window\n",
    "        self.transaction_cost = transaction_cost\n",
    "\n",
    "        obs_shape = (self.lookback * self.data.shape[1],)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=obs_shape, dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(3)  # Hold, Buy, Sell\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.current_step = self.lookback  # start after enough history\n",
    "        obs = self._get_obs()\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        current_close = self.data.iloc[self.current_step]['Close']\n",
    "        next_close = self.data.iloc[self.current_step + 1]['Close']\n",
    "        delta = next_close - current_close\n",
    "        delta_pct = delta / current_close\n",
    "\n",
    "        # Threshold for \"stable\"\n",
    "        threshold_pct = 0.001\n",
    "\n",
    "        # Reward system with stronger penalties\n",
    "        if action == 1:  # Buy\n",
    "            if delta_pct > 0:\n",
    "                reward = 1\n",
    "                if delta_pct > 0.01:  # > 1% gain\n",
    "                    reward = 1.5       # bigger reward for catching strong move\n",
    "            else:\n",
    "                reward = -1\n",
    "                if delta_pct < -0.01:  # > 1% drop\n",
    "                    reward = -1.5      # harsher penalty\n",
    "        elif action == 2:  # Sell\n",
    "            if delta_pct < 0:\n",
    "                reward = 1\n",
    "                if delta_pct < -0.01:  # > 1% drop\n",
    "                    reward = 1.5       # stronger reward\n",
    "            else:\n",
    "                reward = -1\n",
    "                if delta_pct > 0.01:   # > 1% rise\n",
    "                    reward = -1.5      # stronger penalty\n",
    "        else:  # Hold\n",
    "            if abs(delta_pct) < threshold_pct:  # very small move\n",
    "                reward = 0.2   # slight positive for being safe\n",
    "            elif abs(delta_pct) < 0.01:  # small noise\n",
    "                reward = 0\n",
    "            else:  # missed a big move\n",
    "                reward = -1\n",
    "\n",
    "        info = {\n",
    "            'step': self.current_step,\n",
    "            'delta': delta,\n",
    "            'delta_pct': delta_pct,\n",
    "            'threshold_pct': threshold_pct,\n",
    "            'reward': reward,\n",
    "            'action': action\n",
    "        }\n",
    "\n",
    "        self.current_step += 1\n",
    "        terminated = self.current_step >= len(self.data) - 1\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._get_obs() if not terminated else np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        start = self.current_step - self.lookback\n",
    "        end = self.current_step\n",
    "        window = self.data.iloc[start:end].values\n",
    "        return window.flatten().astype(np.float32)\n",
    "\n",
    "# =========================\n",
    "# Evaluation function\n",
    "# =========================\n",
    "def evaluate_model(model, vec_env):\n",
    "    obs = vec_env.reset()\n",
    "    terminated = False\n",
    "    total_steps = 0\n",
    "    num_correct = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    print(\"Starting evaluation episode\")\n",
    "    while not terminated:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, dones, infos = vec_env.step(action)\n",
    "        terminated = dones[0]\n",
    "        info = infos[0]\n",
    "        reward = rewards[0]\n",
    "        action_val = action[0]\n",
    "\n",
    "        delta_pct = info['delta_pct'] * 100\n",
    "        step = info['step']\n",
    "        action_str = ['Hold', 'Buy', 'Sell'][action_val]\n",
    "        if abs(delta_pct) < info['threshold_pct'] * 100:\n",
    "            movement = 'stable'\n",
    "        elif delta_pct > 0:\n",
    "            movement = 'up'\n",
    "        else:\n",
    "            movement = 'down'\n",
    "\n",
    "        print(f\"Step {step}, chosen action {action_str}, next price movement {movement} ({delta_pct:.2f}%), reward {reward:.4f}\")\n",
    "\n",
    "        total_steps += 1\n",
    "        total_reward += reward\n",
    "        if reward >= 0:\n",
    "            num_correct += 1\n",
    "\n",
    "    accuracy = (num_correct / total_steps) * 100 if total_steps > 0 else 0\n",
    "    print(\"Evaluation completed.\")\n",
    "    print(f\"Final evaluation accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Total cumulative reward: {total_reward:.4f}\")\n",
    "    return total_reward\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('BTC-USD_with_Indicators.csv')\n",
    "    \n",
    "    # Split data for training and validation\n",
    "    train_size = int(0.8 * len(df))\n",
    "    train_df = df[:train_size]\n",
    "    val_df = df[train_size:]\n",
    "    \n",
    "    env = BTCTradingEnv(train_df, lookback_window=12, transaction_cost=0.001)\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    val_env = DummyVecEnv([lambda: BTCTradingEnv(val_df, lookback_window=12, transaction_cost=0.001)])\n",
    "    \n",
    "    model_path = 'btc_ppo_agent.zip'\n",
    "    trained = False\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        model = PPO.load(model_path, env=vec_env)\n",
    "        print(\"Loaded existing model.\")\n",
    "    else:\n",
    "        def objective(trial):\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "            clip_range = trial.suggest_float(\"clip_range\", 0.1, 0.3)\n",
    "            \n",
    "            model = PPO(\n",
    "                \"MlpPolicy\",\n",
    "                vec_env,\n",
    "                learning_rate=learning_rate,\n",
    "                clip_range=clip_range,\n",
    "                policy_kwargs={\"net_arch\": {\"pi\": [256, 256], \"vf\": [256, 256]}},\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            steps_per_episode = len(train_df) - 1\n",
    "            total_timesteps = 3 * steps_per_episode  # 3 episodes per trial\n",
    "            \n",
    "            model.learn(total_timesteps=total_timesteps)\n",
    "            \n",
    "            total_reward = evaluate_model(model, val_env)\n",
    "            return total_reward\n",
    "        \n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=4)  # 4 trials\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        \n",
    "        model = PPO(\n",
    "            \"MlpPolicy\",\n",
    "            vec_env,\n",
    "            learning_rate=best_params[\"learning_rate\"],\n",
    "            clip_range=best_params[\"clip_range\"],\n",
    "            policy_kwargs={\"net_arch\": {\"pi\": [256, 256], \"vf\": [256, 256]}},\n",
    "            verbose=1\n",
    "        )\n",
    "        callback = CustomCallback()\n",
    "\n",
    "        steps_per_episode = len(train_df) - 1\n",
    "        total_timesteps = 50 * steps_per_episode\n",
    "\n",
    "        model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "        model.save(model_path)\n",
    "        print(\"Trained and saved new model.\")\n",
    "        trained = True\n",
    "\n",
    "        plt.plot(callback.episode_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Training Reward History')\n",
    "        plt.show()\n",
    "\n",
    "    evaluate_model(model, vec_env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
